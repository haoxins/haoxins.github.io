---
title: Parallel and High Performance Computing
description: 七八个星天外, 两三点雨山前. 旧时茅店社林边, 路转溪桥忽见.
date: 2021-12-01
---

* [Parallel and High Performance Computing](https://www.manning.com/books/parallel-and-high-performance-computing)

# CPU: The parallel workhorse

## Vectorization: FLOPs for free

* In the SIMD case, as the name indicates, there is
  a single instruction that is executed across
  multiple data streams.
  - One vector add instruction replaces eight individual
    scalar add instructions in the instruction queue,
    which reduces the pressure on the instruction
    queue and cache.
  - The biggest benefit is that it takes about the same
    power to perform eight additions in a vector unit
    as one scalar addition.

* Let's briefly summarize **vectorization** terminology:
  - **Vector (SIMD) lane**:
  - A pathway through a vector operation on vector
    registers for a single data element much like
    a lane on a multi-lane freeway.
  - **Vector width**
  - The width of the vector unit, usually expressed in bits.
  - **Vector length**
  - The number of data elements that can be processed
    by the vector in one operation.
  - **Vector (SIMD) instruction sets**
  - The set of instructions that extend the regular
    scalar processor instructions to utilize
    the vector processor.
* **Vectorization** is produced through both a software
  and a hardware component. The requirements are:
  - **Generate instructions**
  - The vector instructions must be generated by the
    compiler or manually specified through
    intrinsics or assembler coding.
  - **Match instructions to the vector unit of the processor**
  - If there is a mismatch between the instructions and the
    hardware, newer hardware can usually process the
    instructions, but older hardware will just fail to run.

> - When you use the latest processors, make sure to use
    the latest versions of the compiler.

* The **vector hardware releases** over the last decade
  have dramatically improved vector functionality.
  - `SSE` (Streaming SIMD Extensions)
  - `SSE2` (From `2001`)
  - `AVX` (Advanced Vector Extensions)
  - `AVX2`
  - `AVX512`
* From the years `2018` and on, **Intel** and **AMD**
  have created multiple variants of `AVX512` as
  incremental improvements to
  **vector hardware architectures**.
* There are several ways to achieve vectorization
  in your program. In ascending order of
  programmer effort, these include:
  - Optimized libraries
  - Auto-vectorization
  - Hints to the compiler
  - Vector intrinsics
  - Assembler instructions

# GPUs: Built to accelerate

## GPU architectures and concepts

* We will use the terminology established by the
  **OpenCL standard** because it was agreed to by
  multiple GPU vendors.
* We will also note alternate terminology that is
  in common use, such as that used by `NVIDIA`.
* Let's look at a few definitions before
  continuing our discussion:
  - **CPU**: The main processor that is installed
    in the socket of the motherboard.
  - **CPU RAM**: The "memory sticks" or dual in-line
    memory modules (DIMMs) containing
    *Dynamic Random-Access Memory* (**DRAM**)
    that are inserted into the memory
    slots in the motherboard.
  - **GPU**: A large peripheral card installed in
    a *Peripheral Component Interconnect Express*
    (**PCIe**) slot on the motherboard.
  - **GPU RAM**: Memory modules on the GPU peripheral
    card for exclusive use of the GPU.
  - **PCI bus**: The wiring that connects the
    peripheral cards to the other components
    on the motherboard.

* Compared to CPUs that can handle tens of
  parallel threads or processes in a clock cycle,
* GPUs are capable of processing thousands of
  parallel threads simultaneously.
* To implement algorithms on GPUs, programmers had to
  reframe their algorithms in terms of these operations,
  which was `time-consuming` and `error-prone`.
* Extending the use of the graphics processor to
  non-graphics workloads became known as general-purpose
  graphics processing unit (**GPGPU**) computing.
* The continued interest and success of `GPGPU` computing
  led to the introduction of a flurry of GPGPU languages.
  - The first to gain wide adoption was the
    Compute Unified Device Architecture (**CUDA**)
    programming language for NVIDIA GPUs, which was
    first introduced in `2007`.
  - The dominant open standard GPGPU computing language
    is the Open Computing Language (OpenCL), developed by
    a group of vendors led by Apple and released in 2009.
* GPUs come in two flavors:
  - **Integrated GPUs**: A graphics processor engine that
    is contained on the CPU
  - **Dedicated GPUs**: A GPU contained on a separate
    peripheral card

---

* In order for work to be executed on the GPU,
  at some point, data must be transferred from
  the CPU to the GPU. When the work is complete,
  and the results are going to be written to file,
  the GPU must send data back to the CPU.
* The instructions the GPU must execute are also
  sent from CPU to GPU. Each one of these transactions
  is mediated by the PCI bus. Although we won't discuss
  how to make these actions happen in this chapter,
  we'll discuss the hardware performance limitations
  of the PCI bus.
* Due to these limitations, a poorly designed GPU
  application can potentially have worse performance
  than that with CPU-only code.
* We'll also discuss the internal architecture of
  the GPU and the performance of the GPU with regards
  to memory and floating-point operations.

* For those of us who have done thread programming
  over the years on a CPU, the graphics processor is
  like the ideal thread engine. The components of
  this thread engine are:
  - A seemingly infinite number of threads
  - Zero time cost for switching or starting threads
  - Latency hiding of memory accesses through
    automatic switching between work groups

* GPU hardware replication units by vendor
  - **AMD**: Shader Engine (SE)
  - **NVIDIA/CUDA**: Graphics processing cluster
* A GPU is composed of
  - GPU RAM (also known as global memory)
  - Workload distributor
  - Compute units (CUs) (SMs in CUDA)
* **CUs** have their own internal architecture,
  often referred to as the *microarchitecture*.
* Instructions and data received from the CPU
  are processed by the workload distributor.
  The distributor coordinates instruction execution
  and data movement onto and off of the CUs.
  The achievable performance of a GPU depends on:
  - Global memory bandwidth
  - Compute unit bandwidth
  - The number of CUs
* A GPU compute device has multiple CUs.
  - **CU**, compute unit, is the term agreed to by
    the community for the OpenCL standard.
  - NVIDIA calls them *streaming multiprocessors*
    (**SMs**),
  - and Intel refers to them as *subslices*.
* Each **CU** contains multiple graphics processors
  called *processing elements* (**PEs**) in OpenCL,
  or *CUDA cores* (or *Compute Cores*) as
  NVIDIA calls them.
  - Intel refers to them as *execution units* (EUs),
  - and the graphics community calls them
    *shader processors*.
* Within each **PE**, it might be possible to perform
  an operation on more than one data item. Depending
  on the details of the GPU microprocessor architecture
  and the GPU vendor, these are referred to as
  `SIMT`, `SIMD`, or *vector operations*.
  - A similar type of functionality can be provided
    by ganging PEs together.

* A typical **GPU** has **different types of memory**.
* The list of the GPU memory types and
  their properties are as follows.
  - **Private memory (register memory)**: Immediately
    accessible by a single PE and only by that PE.
  - **Local memory**: Accessible to a single CU and
    all of the PEs on that CU. Local memory can be
    split between a scratchpad that can be used as a
    programmable cache and, by some vendors, a
    traditional cache on GPUs.
  - Local memory is around `64-96` KB in size.
  - **Constant memory**: Read-only memory accessible
    and shared across all of the CUs.
  - **Global memory**: Memory that's located on the
    GPU and accessible by all of the CUs.

## GPU programming model

* Data decomposition
* Chunk-sized work for processing with some
  shared, local memory
* Operating on multiple data items with a single instruction
* Vectorization (on some GPUs)

* One thing to note from these GPU parallel abstractions
  is that there are fundamentally three, or maybe four,
  **different levels of parallelization** that you can
  apply to a computational loop.

* The technique of **data decomposition** is at the
  heart of how GPUs obtain performance. GPUs break up
  the problem into many smaller blocks of data.
  Then they break it up again, and again.
* If you only have a single instruction stream on a
  single piece of data, a GPU will be slow because it
  has no way to hide the latency. But if you have lots
  of data to operate on, it's incredibly fast.
* To further optimize the graphics operations, GPUs
  recognize that the same operations can be performed
  on many data elements.
  - GPUs are therefore optimized by working on sets of
    data with a single instruction rather than with
    separate instructions for each.
  - This reduces the number of instructions that need
    to be handled. This technique on the CPU is called
    single instruction, multiple data (SIMD).
  - All GPUs emulate this with a group of threads where
    it is called single instruction, multithread (SIMT).
* Because SIMT simulates SIMD operations, it is not
  necessarily constrained the same way as are SIMD
  operations by the underlying vector hardware.
  - Current SIMT operations are executed in lockstep,
    with every thread in the subgroup executing all
    paths through branching if any one thread must
    go through a branch.
  - This is similar to how a SIMD operation is done
    with a mask. But because the SIMT operation is
    emulated, this could be relaxed with more flexibility
    in the instruction pipeline, where more than one
    instruction could be supported.
* The basic unit of operation is called a work item
  in OpenCL. This work item can be mapped to a thread
  or to a processing core, depending on the hardware
  implementation.
* In **CUDA**, it is simply called a **thread** because
  that is how it is mapped in NVIDIA GPUs. Calling it a
  thread is mixing the programming model with how it is
  implemented in the hardware, but it is a little
  clearer to the programmer.
* A work item can invoke another level of parallelism on
  GPUs with vector hardware units. This model of operation
  also maps to the CPU where a thread can
  execute a vector operation.

---

> For convenience and generality, we call the
  **CPU** the **host** and we use the term
  **device** to refer to the **GPU**.

* The GPU programming model splits the `loop body`
  from the `array range` or `index set` that is applied
  to the function. The `loop body` creates the GPU kernel.
  The `index set` and arguments will be used on the
  host to make the kernel call.

* Extract the parallel kernel
* Map from the local data tile to global data
* Calculate data decomposition on the
  host into blocks of data
* Allocate any required memory

* GPU programming is the perfect language for
  the "Me" generation. In the kernel, everything
  is relative to yourself. Take for example

```c
c[i] = a[i] + scalar * b[i];
```

* In this expression, there is no information about the
  extent of the loop. This could be a loop where `i`,
  the global `i` index, covers a range from `0` to
  `1,000` or just the single value `22`.
* Each data item knows what needs to be done to itself
  and itself only. This is truly a "Me" programming model,
  where I care only about myself. What is so powerful
  about this is that the operations on each data element
  become completely independent.
* Let's look at the more complicated example of the
  stencil operator. Although we have two indices, both
  `i` and `j`, and some of the references are to adjacent
  data values, this line of code is still fully defined
  once we determine the values of `i` and `j`.

```c
xnew[j][i] = (x[j][i] + x[j][i-1] + x[j][i+1] + x[j-1][i] + x[j+1][i]) / 5.0;
```

* The key to how the kernel can compose its local
  operation is that, as a product of the data
  decomposition, we provide each work group
  with some information about where it is in the
  local and global domains. In OpenCL, you can get
  the following information:
  - **Dimension**: Gets the number of dimensions,
    either `1D`, `2D`, or `3D`, for this kernel
    from the kernel invocation
  - **Global information**: Global index in each
    dimension, which corresponds to a local work
    unit, or the global size in each dimension,
    which is the size of the global computational
    domain in each dimension
  - **Local (tile) information**: The local size in
    each dimension, which corresponds to the tile
    size in this dimension, or the local index in
    each dimension, which corresponds to the
    tile index in this dimension
  - **Group information**: The number of groups in
    each dimension, which corresponds to the number
    of groups in this dimension, or the group index
    in each dimension, which corresponds to the
    group index in this dimension
* Similar information is available in CUDA, but the
  global index must be calculated from the
  local thread index plus the block (tile) information:

```c
gid = blockIdx.x * blockDim.x + threadIdx.x;
```

* Avoiding `out-of-bound` reads and writes is
  important in GPU kernels because they lead to
  random kernel crashes with
  no error message or output.

* How to address memory resources in
  your GPU programming model?

> On the GPU, the memory coalescing is done at the
  hardware level in the memory controller. The
  performance gains from these coalesced loads
  are substantial.
> But also important is that a lot of the
  optimizations from earlier GPU programming guides
  are no longer necessary, significantly reducing
  the GPU programming effort.

* The basic nature of work on GPUs is **asynchronous**.
  Work is queued up on the GPU and, usually, only
  gets executed when a result or synchronization
  is requested.

## GPU languages: Getting down to basics

* A GPU programming language must have several
  basic features. It is helpful to understand
  what these features are so that you can
  recognize these in each GPU language.
* We summarize the necessary GPU language features here.
  - **Detecting the accelerator device**
  - The language must provide a detection of the
    accelerator devices and a way to choose between
    those devices. Some languages give more control
    over the selection of devices than others. Even
    for a language such as `CUDA`, which just looks
    for an NVIDIA GPU, there must be a way to handle
    multiple GPUs on a node.
  - **Support for writing device kernels**
  - The language must provide a way to generate the
    low-level instructions for GPUs or other accelerators.
    GPUs provide nearly identical basic operations
    as CPUs, so the kernel language should not be
    dramatically different.
  - Rather than invent a new language, the most
    straightforward way is to leverage current
    programming languages and compilers to generate
    the new instruction set. GPU languages have done
    this by adopting a particular version of the
    C or C++ language as a basis for their system.
  - **CUDA** originally was based on the `C`
    programming language but now is based on `C++` and
    has some support for the Standard Template Library (STL).
  - OpenCL is based on the `C99` standard and has
    released a new specification with `C++` support.
  - **Mechanism to call device kernels from the host**
  - Ok, now we have the device code, but we also have to
    have a way of calling the code from the host. The
    syntax for performing this operation varies the most
    across the various languages. But the mechanism is
    only slightly more complicated than a
    standard subroutine call.
  - **Memory handling**
  - The language must have support for memory
    allocations, deallocations, and moving data back
    and forth from the host to the device. The most
    straightforward way for this is to have a subroutine
    call for each of these operations.
  - But another way is through the compiler detecting
    when to move the data and doing it for you behind
    the scenes. As this is such a major part of GPU
    programming, innovation continues to occur on the
    hardware and software side for this functionality.
  - **Synchronization**
  - A mechanism must be provided to specify the
    synchronization requirements between the CPU
    and the GPU. Synchronization operations must also
    be provided within kernels.
  - **Streams**
  - A complete GPU language allows the scheduling
    of asynchronous streams of operations along with
    the explicit dependencies between the kernels and
    the memory transfer operations.

# High performance computing ecosystems

## Affinity: Truce with the kernel

* We first encountered **affinity** on the **MPI**
  (*Message Passing Interface*), where we defined
  it and briefly showed how to handle it.
* We repeat the definition here and also
  define process placement.
  - **Affinity**: Assigns a preference for the
    scheduling of a process, rank or thread to
    a particular hardware component. This is also
    called **pinning** or **binding**.
  - **Placement**: Assigns a process or thread
    to a hardware location.
* It is not enough to keep every parallel process
  active and scheduled. We also need to keep
  processes scheduled on the same
  *Non-Uniform Memory Access* (NUMA) domain to
  minimize memory access costs.

---

* The concept of **affinity** is born out of how the
  operating system sees things. At the level of the
  operating system, you can set where each process
  is allowed to run.
  - On Linux, this is done through either the
    `taskset` or the `numactl` commands. These commands,
    and similar utilities on other operating systems,
    emerged as the complexity of the CPU grew so that
    you could provide more information to the scheduler
    in the operating system.
  - The directions might be taken as hints or requirements
    by the scheduler. Using these commands, you can pin
    a server process to a particular processor to be
    closer to a particular hardware component or to
    gain faster response.
  - This focus on affinity alone is enough when dealing
    with a single process.
* For parallel programming, there are additional
  considerations. We have a set of processes that we
  need to consider. Lets say we have `16` processors and
  we are running a `four` rank MPI job.
  - Where do we put the ranks?
  - Do we put these across the sockets, on all the sockets,
    pack them close together, or spread them out?
  - Do we place certain ranks next to each other
    (ranks `1` and `2` together or ranks `1` and `4` together)?
* To be able to answer these questions, we need to
  address the following:
  - **Mapping**: the placement of processes
  - **Order of ranks**: which ranks are close together
  - **Binding**: affinity or tying a process to
    a location or locations
