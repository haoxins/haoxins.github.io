---
title: 深度学习
description: 二十四桥仍在, 波心荡, 冷月无声. 念桥边红药, 年年知为谁生!
date: 2022-08-02
---

- [深度学习](https://book.douban.com/subject/27087503/)

> 或许对于数学基础性的研究, 会反映为 ML vs DL 的理论能力极限.
  或许吧? 未来可以无责任畅想下!

## 线性代数

- 在深度学习中, 我们也使用一些不那么常规的符号.
  我们允许矩阵和向量相加, 产生另一个矩阵:
  - $$ C = A + b $$,
    其中
    $$ C_{i, j} = A_{i, j} + b_{j} $$.
  - 换言之, 向量 `b` 和矩阵 `A` 的每一行相加.
- 这个简写方法使我们无须在加法操作前定义一个将向量 `b` 复制到每一行而生成的矩阵.
  - 这种隐式地复制向量 `b` 到很多位置的方式, 称为`广播`.

---

- 有时我们需要衡量一个向量的大小.
  - 在机器学习中, 我们经常使用称为`范数` (`norm`) 的函数来衡量向量大小.
  - 形式上,
    $$ L^p $$
    范数定义如下
  - $$ \| x \| _{p} = (\sum_{i} \mid x_i \mid ^{p})^{\frac{1}{p}} $$
  - 其中
    $$ p \in \mathbb{R} $$,
    $$ p \ge 1 $$.
- 范数 (包括
  $$ L^p $$
  范数) 是将向量映射到非负值的函数.
  - 直观上来说, 向量 `x` 的范数衡量从原点到点 `x` 的距离.
  - 更严格地说, 范数是满足下列性质的任意函数:
  - $$ f(x) = 0 \Longrightarrow x = 0 $$;
  - $$ f(x + y) \le f(x) + f(y) $$
    (__三角不等式__);
  - $$ \forall a \in \mathbb{R}, f(ax) = \mid a \mid f(x) $$.
- 当 `p = 2` 时,
  $$ L^2 $$
  范数称为欧几里得范数.
  - 它表示从原点出发到向量 `x` 确定的点的欧几里得距离.
  - $$ L^2 $$
    范数在机器学习中出现得十分频繁, 经常简化表示为
    $$ \| x \| $$,
    略去了下标 `2`.
  - 平方
    $$ L^2 $$
    范数也经常用来衡量向量的大小, 可以简单地通过点积
    $$ x^{\top} x $$
    计算.
- 平方
  $$ L^2 $$
  范数在数学和计算上都比
  $$ L^2 $$
  范数本身更方便.
  - 例如, 平方
    $$ L^2 $$
    范数对 `x` 中每个元素的导数只取决于对应的元素, 而
    $$ L^2 $$
    范数对每个元素的导数和整个向量相关.
  - 但是在很多情况下, 平方
    $$ L^2 $$
    范数也可能不受欢迎, 因为它在原点附近增长得十分缓慢.
  - 在某些机器学习应用中, 区分恰好是零的元素和非零但值很小的元素是很重要的.
  - 在这些情况下, 我们转而使用在各个位置斜率相同,
    同时保持简单的数学形式的函数:
    $$ L^1 $$
    范数.
  - $$ L^1 $$
    范数可以简化如下
  - $$ \| x \|_{1} = \sum_{i} \mid x_{i} \mid $$

---

- 另外一个经常在机器学习中出现的范数是
  $$ L^{\infty} $$
  范数, 也被称为`最大范数` (max norm).
  - 这个范数表示向量中具有最大幅值的元素的绝对值:
  - $$ \| x \|_{\infty} = \underset{i}{max} \mid x_i \mid $$
- 有时候我们可能也希望衡量矩阵的大小. 在深度学习中,
  最常见的做法是使用 `Frobenius 范数` (Frobenius norm), 即
  - $$ \| A \|_{F} = \sqrt{\sum_{i, j} A_{i, j}^2} $$
  - 其类似于向量的
    $$ L^2 $$
    范数.
- 两个向量的点积可以用范数来表示, 具体如下
  - $$ {x}^{\top}{y} = \| x \|_{2} \| y \|_{2} \cos \theta $$
  - 其中
    $$ \theta $$
    表示 `x` 和 `y` 之间的夹角.

---

- 方阵 `A` 的特征向量是指与 `A` 相乘后相当于对该向量进行缩放的非零向量 `v`:
  - $$ \mathbf{A} \mathbf{v} = \lambda \mathbf{v} $$
  - 其中标量
    $$ \lambda $$
    称为这个特征向量对应的特征值.
  - (类似地, 我们也可以定义左特征向量
    $$ \mathbf{v}^{\top} \mathbf{A} = \lambda \mathbf{v}^{\top} $$,
    但是通常我们更关注右特征向量).
- 如果 `v` 是 `A` 的特征向量, 那么任何缩放后的向量
  $$ s \mathbf{v} (s \in \mathbb{R}, s \ne 0) $$
  也是 `A` 的特征向量.
  - 此外, `sv` 和 `v` 有相同的特征值.
  - 基于这个原因, 通常我们只考虑单位特征向量.
- 假设矩阵 `A` 有 `n` 个线性无关的特征向量
  $$ \{ \mathbf{v}^{(1)}, ..., \mathbf{v}^{(n)} \} $$,
  对应着特征值
  $$ \{ \lambda_{1}, ..., \lambda_{n} \} $$.
  - 我们将特征向量连接成一个矩阵, 使得每一列是一个特征向量:
    $$ \mathbf{V} = \left [ \mathbf{v}^{(1)}, ..., \mathbf{v}^{(n)} \right ] $$.
  - 类似地, 我们也可以将特征值连接成一个向量
    $$ \lambda = \left [ \lambda_{1}, ..., \lambda_{n} \right ] ^{\top} $$.
  - 因此 `A` 的特征分解可以记作
    $$ \mathbf{A} = \mathbf{V} diag(\lambda) \mathbf{V}^{-1} $$

---

- 我们已经看到了构建具有特定特征值和特征向量的矩阵,
  能够使我们在目标方向上延伸空间. 然而,
  我们也常常希望将矩阵分解成特征值和特征向量.
  - 这样可以帮助我们分析矩阵的特定性质,
    就像质因数分解有助于我们理解整数.
- 不是每一个矩阵都可以分解成特征值和特征向量.
  在某些情况下, 特征分解存在, 但是会涉及复数而非实数.
  - 幸运的是, 在本书中, 我们通常只需要分解一类有简单分解的矩阵.
- 具体来讲, 每个实对称矩阵都可以分解成实特征向量和实特征值:
  - $$ \mathbf{A} = \mathbf{Q} \mathbf{Λ} \mathbf{Q} ^{\top} $$
  - 其中 `Q` 是 `A` 的特征向量组成的正交矩阵, `Λ` 是对角矩阵.
  - 特征值
    $$ Λ_{i, i} $$
    对应的特征向量是矩阵 `Q` 的第 `i` 列, 记作
    $$ Q_{:, i} $$.
  - 因为 `Q` 是正交矩阵, 我们可以将 `A` 看作沿方向
    $$ v^{(i)} $$
    延展
    $$ \lambda_{i} $$
    倍的空间.

---

- 虽然任意一个实对称矩阵 `A` 都有特征分解, 但是特征分解可能并不唯一.
  - 如果两个或多个特征向量拥有相同的特征值,
    那么在由这些特征向量产生的生成子空间中,
    任意一组正交向量都是该特征值对应的特征向量.
- 因此, 我们可以等价地从这些特征向量中构成 `Q` 作为替代.
  - 按照惯例, 我们通常按降序排列 `Λ` 的元素.
  - 在该约定下, 特征分解唯一, 当且仅当所有的特征值都是唯一的.
  - 矩阵的特征分解给了我们很多关于矩阵的有用信息.

---

- 矩阵是奇异的, 当且仅当含有零特征值.
  实对称矩阵的特征分解也可以用于优化二次方程
  $$ f(x) = x^{\top} \mathbf{A} x $$,
  其中限制
  $$ \| x \|_{2} = 1 $$.
  - 当 `x` 等于 `A` 的某个特征向量时, `f` 将返回对应的特征值.
  - 在限制条件下, 函数 `f` 的最大值是最大特征值, 最小值是最小特征值.
- 所有特征值都是正数的矩阵称为`正定`;
  所有特征值都是非负数的矩阵称为`半正定`.
  - 同样地, 所有特征值都是负数的矩阵称为`负定`;
    所有特征值都是非正数的矩阵称为`半负定`.
- 半正定矩阵受到关注是因为它们保证
  $$ \forall x, x^{\top} \mathbf{A} x ≥ 0 $$.
  - 此外, 正定矩阵还保证
    $$ x^{\top} \mathbf{A} x = 0 \Rightarrow x = 0 $$.

---

- 奇异值分解 (singular value decomposition, SVD),
  是将矩阵分解为奇异向量 (singular vector) 和奇异值 (singular value).
  - 通过奇异值分解, 我们会得到一些与特征分解相同类型的信息.
  - 然而, 奇异值分解有更广泛的应用.
  - 每个实数矩阵都有一个奇异值分解, 但不一定都有特征分解.
  - 例如, 非方阵的矩阵没有特征分解, 这时我们只能使用奇异值分解.
- 回想一下, 我们使用特征分解去分析矩阵 `A` 时,
  得到特征向量构成的矩阵 `V` 和特征值构成的向量
  $$ \lambda $$,
  我们可以重新将 `A` 写作
  - $$ \mathbf{A} = \mathbf{V} diag(\lambda) \mathbf{V}^{-1} $$
- 奇异值分解是类似的, 只不过这回我们将矩阵 `A` 分解成三个矩阵的乘积:
  - $$ \mathbf{A} = \mathbf{U} \mathbf{D} \mathbf{V} ^{\top} $$
  - 假设 `A` 是一个
    $$ m \times n $$
    的矩阵, 那么 `U` 是一个
    $$ m \times m $$
    的矩阵, `D` 是一个
    $$ m \times n $$
    的矩阵, `V` 是一个
    $$ n \times n $$
    矩阵.
- 这些矩阵中的每一个经定义后都拥有特殊的结构.
  矩阵 `U` 和 `V` 都定义为正交矩阵,
  而矩阵 `D` 定义为对角矩阵.
  - 注意, 矩阵 `D` 不一定是方阵.
- 对角矩阵 `D` 对角线上的元素称为矩阵 `A` 的奇异值 (singular value).
  - 矩阵 `U` 的列向量称为左奇异向量 (left singular vector),
    矩阵 `V` 的列向量称右奇异向量 (right singular vector).
- 事实上, 我们可以用与 `A` 相关的特征分解去解释 `A` 的奇异值分解.
  - `A` 的左奇异向量 (left singular vector) 是
    $$ \mathbf{A} \mathbf{A} ^{\top} $$
    的特征向量.
  - `A` 的右奇异向量 (right singular vector) 是
    $$ \mathbf{A} ^{\top} \mathbf{A} $$
    的特征向量.
  - `A` 的非零奇异值是
    $$ \mathbf{A} \mathbf{A} ^{\top} $$
    特征值的平方根, 同时也是
    $$ \mathbf{A} ^{\top} \mathbf{A} $$
    特征值的平方根.

> `SVD` 最有用的一个性质可能是拓展矩阵求逆到非方矩阵上.

---

- 对于非方矩阵而言, 其逆矩阵没有定义.
  假设在下面的问题中, 我们希望通过矩阵 `A` 的左逆
  `B` 来求解线性方程:
  - $$ \mathbf{A} x = y $$
- 等式两边左乘左逆 `B` 后, 我们得到
  - $$ x = \mathbf{B} y $$
- 取决于问题的形式, 我们可能无法设计一个唯一的映射将 `A` 映射到 `B`.
  - 如果矩阵 `A` 的行数大于列数, 那么上述方程可能没有解.
  - 如果矩阵 `A` 的行数小于列数, 那么上述矩阵可能有多个解.

---

- __伪逆__ 使我们在这类问题上取得了一定的进展. 矩阵 `A` 的伪逆定义为

$$
\mathbf{A}^{+} = \lim_{a \searrow 0}
(\mathbf{A}^{\top} \mathbf{A} + a \mathbf{I})^{-1}
\mathbf{A}^{\top}
$$

- 计算伪逆的实际算法没有基于这个定义, 而是使用下面的公式
  - $$ \mathbf{A}^{+} = \mathbf{V} \mathbf{D}^{+} \mathbf{U}^{\top} $$
  - 其中, 矩阵 `U`, `D` 和 `V` 是矩阵 `A` 奇异值分解后得到的矩阵.
  - 对角矩阵 `D` 的伪逆
    $$ D^{+} $$
    是其非零元素取倒数之后再转置得到的.
- 当矩阵 `A` 的列数多于行数时, 使用伪逆求解线性方程是众多可能解法中的一种.
  - 特别地,
    $$ x = \mathbf{A}^{+} y $$
    是方程所有可行解中欧几里得范数
    $$ \| x \|_{2} $$
    最小的一个.
- 当矩阵 `A` 的行数多于列数时, 可能没有解. 在这种情况下, 通过伪逆得到的
  `x` 使得 `Ax` 和 `y` 的欧几里得距离
  $$ \| \mathbf{A} x - y \|_{2} $$
  最小.

---

- 迹运算返回的是矩阵对角元素的和:
  - $$ Tr(A) = \sum_{i} A_{i, i} $$.
- 迹运算提供了另一种描述矩阵 Frobenius 范数的方式:
  - $$ \| A \|_{F} = \sqrt{Tr(A A^{\top})} $$
- 多个矩阵相乘得到的方阵的迹,
  和将这些矩阵中的最后一个挪到最前面之后相乘的迹是相同的.
  - 当然, 我们需要考虑挪动之后矩阵乘积依然定义良好:
  - $$ Tr(ABC) = Tr(CAB) = Tr(BCA) $$
- 或者更一般地,

$$
Tr(\prod_{i = 1}^{n} F^{(i)}) =
Tr(F^{(n)} \prod_{i = 1}^{n - 1} F^{(i)})
$$

- 即使循环置换后矩阵乘积得到的矩阵形状变了,
  迹运算的结果依然不变.

## 概率与信息论

- 期望是线性的, 例如,

$$
\mathbb{E}_{x} \left [ α f(x) + β g(x) \right ] =
α \mathbb{E}_{x} \left [ f(x) \right ] +
β \mathbb{E}_{x} \left [ g(x) \right ]
$$

- 其中 `α` 和 `β` 不依赖于 `x`.

---

- `方差`衡量的是当我们对 `x` 依据它的概率分布进行采样时,
  随机变量 `x` 的函数值会呈现多大的差异:

$$
Var(f(x)) =
\mathbb{E} \left [ (
  f(x) - \mathbb{E} \left [ f(x) \right ]
)^{2} \right ]
$$

- 当方差很小时, `f(x)` 的值形成的簇比较接近它们的期望值.
  - 方差的平方根被称为`标准差`.

---

- `协方差`在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度:

$$
Cov(f(x), g(y)) = \mathbb{E} \left [
  (f(x) - \mathbb{E} \left [ f(x) \right ])
  (g(y) - \mathbb{E} \left [ g(y) \right ])
\right ]
$$

- 协方差的绝对值如果很大, 则意味着变量值变化很大,
  并且它们同时距离各自的均值很远.
  - 如果协方差是正的, 那么两个变量都倾向于同时取得相对较大的值.
  - 如果协方差是负的, 那么其中一个变量倾向于取得相对较大的值的同时,
    另一个变量倾向于取得相对较小的值, 反之亦然.
- 其他的衡量指标如`相关系数`将每个变量的贡献归一化,
  为了只衡量变量的相关性而不受各个变量尺度大小的影响.
- 协方差和相关性是有联系的, 但实际上是不同的概念.
  - 它们是有联系的:
  - 如果两个变量相互独立, 那么它们的协方差为零;
  - 如果两个变量的协方差不为零, 那么它们一定是相关的.
- 然而, 独立性又是和协方差完全不同的性质.
  - 两个变量如果协方差为零, 它们之间一定没有线性关系.
  - 独立性是比零协方差的要求更强, 因为独立性还排除了非线性的关系.
- 两个变量相互依赖, 但是具有零协方差是可能的.
  - 例如, 假设我们首先从区间 `[-1, 1]` 上的均匀分布中采样出一个实数 `x`,
    然后对一个随机变量 `s` 进行采样.
  - `s` 以
    $$ \frac{1}{2} $$
    的概率值为 `1`, 否则为 `-1`.
  - 我们可以通过令 `y = sx` 来生成一个随机变量 `y`.
    显然, `x` 和 `y` 不是相互独立的,
    因为 `x` 完全决定了 `y` 的尺度.
  - 然而, `Cov(x, y) = 0`.
- 随机向量
  $$ x \in \mathbb{R}^{n} $$
  的`协方差矩阵`是一个
  $$ n \times n $$
  的矩阵, 并且满足
  - $$ Cov(x)_{i, j} = Cov(x_i, x_j) $$
  - 协方差矩阵的对角元是方差:
  - $$ Cov(x_i, x_i) = Var(x_i) $$

## 深度前馈网络

```
量化模型的容量使得统计学习理论可以进行量化预测.
统计学习理论中最重要的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量增长而增长,
但随着训练样本增多而下降. 这些边界为机器学习算法可以有效解决问题提供了理论验证,
但是它们很少应用于实际中的深度学习算法. 一部分原因是边界太松,
另一部分原因是很难确定深度学习算法的容量. 由于有效容量受限于优化算法的能力,
确定深度学习模型容量的问题特别困难. 而且对于深度学习中的一般非凸优化问题,
我们只有很少的理论分析.

我们必须记住虽然更简单的函数更可能泛化 (训练误差和测试误差的差距小),
但我们仍然需要选择一个充分复杂的假设以达到低的训练误差.
通常, 当模型容量上升时, 训练误差会下降, 直到其渐近最小可能误差 (假设误差度量有最小值).
通常, 泛化误差是一个关于模型容量的 U 形曲线函数.

为考虑容量任意高的极端情况, 我们介绍非参数模型的概念. 至此, 我们只探讨过参数模型,
例如线性回归. 参数模型学习的函数在观测到新数据前, 参数向量的分量个数是有限且固定的.
非参数模型没有这些限制.

有时, 非参数模型仅是一些不能实际实现的理论抽象 (比如搜索所有可能概率分布的算法).
然而, 我们也可以设计一些实用的非参数模型, 使它们的复杂度和训练集大小有关.
```

```
有时一个选项被设为学习算法不用学习的超参数, 是因为它太难优化了.
更多的情况是, 该选项必须是超参数, 因为它不适合在训练集上学习.
这适用于控制模型容量的所有超参数. 如果在训练集上学习超参数,
这些超参数总是趋向于最大可能的模型容量, 导致过拟合.
例如, 相比低次多项式和正的权重衰减设定,
更高次的多项式和权重衰减参数设定 λ = 0 总能在训练集上更好地拟合.

为了解决这个问题, 我们需要一个训练算法观测不到的验证集样本.
早先我们讨论过和训练数据相同分布的样本组成的测试集,
它可以用来估计学习过程完成之后的学习器的泛化误差.
其重点在于测试样本不能以任何形式参与到模型的选择中, 包括设定超参数.
基于这个原因, 测试集中的样本不能用于验证集.
因此, 我们总是从训练数据中构建验证集.
特别地, 我们将训练数据分成两个不相交的子集.
其中一个用于学习参数. 另一个作为验证集,
用于估计训练中或训练后的泛化误差, 更新超参数.
用于学习参数的数据子集通常仍被称为训练集,
尽管这会和整个训练过程用到的更大的数据集相混.
用于挑选超参数的数据子集被称为验证集.
通常, 80% 的训练数据用于训练, 20% 用于验证.
由于验证集是用来"训练"超参数的, 尽管验证集的误差通常会比训练集误差小,
验证集会低估泛化误差. 所有超参数优化完成之后, 泛化误差可能会通过测试集来估计.
```

- 随机梯度下降的核心是, 梯度是期望. 期望可使用小规模的样本近似估计.
  - 具体而言, 在算法的每一步,
    我们从训练集中均匀抽出一小批量 (minibatch) 样本
  - $$ \mathbb{B} = \left \{ x^{(1)}, ..., x^{(m')} \right \} $$.
  - 小批量的数目 `m'` 通常是一个相对较小的数, 从一到几百.
  - 重要的是, 当训练集大小 `m` 增长时, `m'` 通常是固定的.
  - 我们可能在拟合几十亿的样本时, 每次更新计算只用到几百个样本.

```
尽管术语"流形"有正式的数学定义, 但是机器学习倾向于更松散地定义一组点,
只需要考虑少数嵌入在高维空间中的自由度或维数就能很好地近似.
每一维都对应着局部的变化方向.
在机器学习中, 我们允许流形的维数从一个点到另一个点有所变化.
这经常发生于流形和自身相交的情况中.
例如, 数字"8"形状的流形在大多数位置只有一维, 但在中心的相交处有两维.
```

- 具体来说, 万能近似定理表明,
  一个前馈神经网络如果具有线性输出层和至少一层具有任何一种"挤压"性质的激活函数
  (例如 logistic sigmoid 激活函数) 的隐藏层, 只要给予网络足够数量的隐藏单元,
  它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的 Borel 可测函数.
  前馈网络的导数也可以任意好地来近似函数的导数.
  - Borel 可测的概念超出了本书的范畴.

---

- 总之, 具有单层的前馈网络足以表示任何函数, 但是网络层可能大得不可实现,
  并且可能无法正确地学习和泛化.
  - 在很多情况下, 使用更深的模型能够减少表示期望函数所需的单元的数量,
    并且可以减少泛化误差.
- 存在一些函数族能够在网络的深度大于某个值 `d` 时被高效地近似,
  而当深度被限制到小于或等于 `d` 时需要一个远远大于之前的模型.
  - 在很多情况下, 浅层模型所需的隐藏单元的数量是 `n` 的指数级.

---

- 反向传播这个术语经常被误解为用于多层神经网络的整个学习算法.
  实际上, 反向传播仅指用于计算梯度的方法, 而另一种算法,
  例如随机梯度下降, 使用该梯度来进行学习.
  - 此外, 反向传播经常被误解为仅适用于多层神经网络,
    但是原则上它可以计算任何函数的导数
    (对于一些函数, 正确的响应是报告函数的导数是未定义的).

## 深度学习中的正则化

```
深度学习界在某种程度上已经与更广泛的计算机科学界隔离开来,
并且在很大程度上发展了自己关于如何进行微分的文化态度.
一般来说, 自动微分 (automatic differentiation)
领域关心如何以算法方式计算导数.
这里描述的反向传播算法只是自动微分的一种方法.
它是一种称为反向模式累加 (reverse mode accumulation)
的更广泛类型的技术的特殊情况. 其他方法以不同的顺序来计算链式法则的子表达式.
一般来说, 确定一种计算的顺序使得计算开销最小, 是困难的问题.
找到计算梯度的最优操作序列是 NP 完全问题, 在这种意义上,
它可能需要将代数表达式简化为它们最廉价的形式.
```

```
其中一个算法上的变化是用交叉熵族损失函数替代均方误差损失函数.
均方误差在 20 世纪 80 年代和 90 年代流行, 但逐渐被交叉熵损失替代,
并且最大似然原理的想法在统计学界和机器学习界之间广泛传播.
使用交叉熵损失大大提高了具有 sigmoid 和 softmax 输出的模型的性能,
而当使用均方误差损失时会存在饱和和学习缓慢的问题.
```

```
Dropout 训练与 Bagging 训练不太一样.
在 Bagging 的情况下, 所有模型都是独立的.
在 Dropout 的情况下, 所有模型共享参数,
其中每个模型继承父神经网络参数的不同子集.
参数共享使得在有限可用的内存下表示指数级数量的模型变得可能.
在 Bagging 的情况下, 每一个模型在其相应训练集上训练到收敛.
在 Dropout 的情况下, 通常大部分模型都没有显式地被训练,
因为通常父神经网络会很大, 以至于到宇宙毁灭都不可能采样完所有的子网络.
取而代之的是, 在单个步骤中我们训练一小部分的子网络,
参数共享会使得剩余的子网络也能有好的参数设定. 这些是仅有的区别.
除了这些, Dropout 与 Bagging 算法一样.
例如, 每个子网络中遇到的训练集确实是有放回采样的原始训练集的一个子集.
```

## 深度模型中的优化

```
另一个促使我们从小数目样本中获得梯度的统计估计的动机是训练集的冗余.
在最坏的情况下, 训练集中所有的 m 个样本都是彼此相同的拷贝.
基于采样的梯度估计可以使用单个样本计算出正确的梯度,
而比原来的做法少花了 m 倍时间. 实践中, 我们不太可能真的遇到这种最坏情况,
但可能会发现大量样本都对梯度做出了非常相似的贡献.
```

```
更大的初始权重具有更强的破坏对称性的作用, 有助于避免冗余的单元.
它们也有助于避免在每层线性成分的前向或反向传播中丢失信号 --
矩阵中更大的值在矩阵乘法中有更大的输出.
如果初始权重太大, 那么会在前向传播或反向传播中产生爆炸的值.
在循环网络中, 很大的权重也可能导致混沌 (chaos)
(对于输入中很小的扰动非常敏感, 导致确定性前向传播过程表现随机).
在一定程度上, 梯度爆炸问题可以通过梯度截断来缓解
(执行梯度下降步骤之前设置梯度的阈值).
较大的权重也会产生使得激活函数饱和的值, 导致饱和单元的梯度完全丢失.
这些竞争因素决定了权重的理想初始大小.
```

## 卷积网络

```
卷积是一种特殊的线性运算.
卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络.
```

> ML 中的卷积运算和数学意义上的卷积有差异~

```
卷积运算可交换性的出现是因为我们将核相对输入进行了翻转,
从 m 增大的角度来看, 输入的索引在增大, 但是核的索引在减小.
我们将核翻转的唯一目的是实现可交换性. 尽管可交换性在证明时很有用,
但在神经网络的应用中却不是一个重要的性质. 与之不同的是,
许多神经网络库会实现一个相关的函数, 称为互相关函数,
和卷积运算几乎一样但是并没有对核进行翻转.

许多机器学习的库实现的是互相关函数但是称之为卷积.
在这本书中我们遵循把两种运算都叫作卷积的这个传统,
在与核翻转有关的上下文中, 我们会特别指明是否对核进行了翻转.
```

```
在机器学习中, 学习算法会在核合适的位置学得恰当的值,
所以一个基于核翻转的卷积运算的学习算法所学得的核,
是对未进行翻转的算法学得的核的翻转.
单独使用卷积运算在机器学习中是很少见的, 卷积经常与其他的函数一起使用,
无论卷积运算是否对它的核进行了翻转, 这些函数的组合通常是不可交换的.
```

```
处于卷积网络更深的层中的单元, 它们的接受域要比处在浅层的单元的接受域更大.
如果网络还包含类似步幅卷积或者池化之类的结构特征, 这种效应会加强.
这意味着在卷积网络中尽管直接连接都是很稀疏的,
但处在更深的层中的单元可以间接地连接到全部或者大部分输入图像.
```

```
池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出.
例如, 最大池化函数给出相邻矩形区域内的最大值.
其他常用的池化函数包括相邻矩形区域内的平均值,
L2 范数以及基于距中心像素距离的加权平均函数.
```

```
使用池化可以看作增加了一个无限强的先验:
这一层学得的函数必须具有对少量平移的不变性.
当这个假设成立时, 池化可以极大地提高网络的统计效率.
对空间区域进行池化产生了平移不变性,
但当我们对分离参数的卷积的输出进行池化时,
特征能够学得应该对于哪种变换具有不变性.
```

```
我们可以把卷积网络类比成全连接网络, 但对于这个全连接网络的权重有一个无限强的先验.
这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同, 但可以在空间上移动.
这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重以外,
其余的权重都为零.
总之, 我们可以把卷积的使用当作对网络中一层的参数引入了一个无限强的先验概率分布.
这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性.
类似地, 使用池化也是一个无限强的先验: 每一个单元都具有对少量平移的不变性.
```

```
当我们知道每一个特征都是一小块空间的函数并且相同的特征不会出现在所有的空间上时,
局部连接层是很有用的.
```

```
平铺卷积对卷积层和局部连接层进行了折衷.
这里并不是对每一个空间位置的权重集合进行学习,
我们学习一组核使得当我们在空间移动时它们可以循环利用.
这意味着在近邻的位置上拥有不同的过滤器, 就像局部连接层一样,
但是对于这些参数的存储需求仅仅会增长常数倍,
这个常数就是核的集合的大小, 而不是整个输出的特征映射的大小.
```

- 卷积等效于使用傅里叶变换将输入与核都转换到频域, 执行两个信号的逐点相乘,
  再使用傅里叶逆变换转换回时域. 对于某些问题的规模,
  这种算法可能比离散卷积的朴素实现更快.
- 当一个 `d` 维的核可以表示成 `d` 个向量 (每一维一个向量) 的外积时,
  该核被称为可分离的. 当核可分离时, 朴素的卷积是低效的.
  它等价于组合 `d` 个一维卷积, 每个卷积使用这些向量中的一个.
  - 组合方法显著快于使用它们的外积来执行一个 `d` 维的卷积,
    并且核也只要更少的参数来表示成向量.
  - 如果核在每一维都是 `w` 个元素宽, 那么朴素的多维卷积需要
    $$ O(w^d) $$
    的运行时间和参数存储空间, 而可分离卷积只需要
    $$ O(w \times d) $$
    的运行时间和参数存储空间.
  - 当然, 并不是每个卷积都可以表示成这种形式.

## 序列建模: 循环和递归网络

```
具体来说, 每当模型能够表示长期依赖时,
长期相互作用的梯度幅值就会变得指数小 (相比短期相互作用的梯度幅值).
这并不意味着这是不可能学习的,
由于长期依赖关系的信号很容易被短期相关性产生的最小波动隐藏,
因而学习长期依赖可能需要很长的时间.
实践中, 实验表明, 当我们增加了需要捕获的依赖关系的跨度,
基于梯度的优化变得越来越困难, SGD 在长度仅为 10 或 20
的序列上成功训练传统 RNN 的概率迅速变为 0.
```

## 实践方法论

- 梯度截断特别重要, 因为它保持了爆炸梯度边缘的 `RNN` 动态.
  如果没有梯度截断, 梯度爆炸将阻碍学习的成功.

| 超参数 | 容量何时增加 | 原因 | 注意事项 |
|-------|:----------:|:---:|:-------:|
| 隐藏单元数量 | 增加 | 增加隐藏单元数量会增加模型的表示能力 | 几乎模型每个操作所需的时间和内存代价都会随隐藏单元数量的增加而增加 |
| 学习率 | 调至最优 | 不正确的学习速率, 不管是太高还是太低都会由于优化失败而导致低有效容量的模型 | |
| 卷积核宽度 | 增加 | 增加卷积核宽度会增加模型的参数数量 | 较宽的卷积核导致较窄的输出尺寸, 除非使用隐式零填充减少此影响, 否则会降低模型容量. 较宽的卷积核需要更多的内存存储参数, 并会增加运行时间, 但较窄的输出会降低内存代价 |
| 隐式零填充 | 增加 | 在卷积之前隐式添加零能保持较大尺寸的表示 | 大多数操作的时间和内存代价会增加 |
| 权重衰减系数 | 降低 | 降低权重衰减系数使得模型参数可以自由地变大 | |
| Dropout 比率 | 降低 | 较少地丢弃单元可以更多地让单元彼此"协力"来适应训练集 | |

```
实际上, 当有几个超参数对性能度量没有显著影响时,
随机搜索相比于网格搜索指数级地高效.
相比于网格搜索, 随机搜索能够更快地减小验证集误差
(就每个模型运行的试验数而言).
与网格搜索一样, 我们通常会重复运行不同版本的随机搜索,
以基于前一次运行的结果改进下一次搜索.
```

## 表示学习

- 从自编码器获得有用特征的一种方法是限制 `h` 的维度比 `x` 小,
  这种编码维度小于输入维度的自编码器称为`欠完备`自编码器.
  - 学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征.

```
因此, 拥有非线性编码器函数 f 和非线性解码器函数 g
的自编码器能够学习出更强大的 PCA 非线性推广.
不幸的是, 如果编码器和解码器被赋予过大的容量,
自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息.
```

```
理想情况下, 根据要建模的数据分布的复杂性, 选择合适的编码维数和编码器,
解码器容量, 就可以成功训练任意架构的自编码器. 正则自编码器提供这样的能力.
正则自编码器使用的损失函数可以鼓励模型学习其他特性 (除了将输入复制到输出),
而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量.
这些特性包括稀疏表示, 表示的小导数以及对噪声或输入缺失的鲁棒性.
即使模型容量大到足以学习一个无意义的恒等函数,
非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布的有用信息.
```

## 深度学习中的结构化概率模型

- `有向图模型`是一种结构化概率模型, 也被称为`信念网络`或者`贝叶斯网络`.
- `有向图模型`为我们提供了一种描述结构化概率模型的语言.
  而另一种常见的语言则是`无向模型`, 也被称为`马尔可夫随机场`或者是`马尔可夫网络`.
  - 就像它们的名字所说的那样, 无向模型中所有的边都是没有方向的.

```
当存在很明显的理由画出每一个指向特定方向的箭头时, 有向模型显然最适用.
有向模型中, 经常存在我们理解的具有因果关系以及因果关系有明确方向的情况.
```

```
虽然玻尔兹曼机最初的定义既可以包含潜变量, 也可以不包含潜变量,
但是时至今日玻尔兹曼机这个术语通常用于指拥有潜变量的模型,
而没有潜变量的玻尔兹曼机则经常被称为马尔可夫随机场或对数线性模型.
```

```
换句话说, 基于能量的模型只是一种特殊的马尔可夫网络:
求幂使能量函数中的每个项对应于不同团的一个因子.
```

------------------

- [深度强化学习图解](https://book.douban.com/subject/36019621/)

> 不推荐! 原书一般, 翻译较差~
