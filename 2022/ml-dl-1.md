---
title: 深度学习
description: 二十四桥仍在, 波心荡, 冷月无声. 念桥边红药, 年年知为谁生!
date: 2022-08-02
---

- [深度学习](https://book.douban.com/subject/27087503/)

> 或许对于数学基础性的研究, 会反映为 ML vs DL 的理论能力极限.
  或许吧? 未来可以无责任畅想下!

## 线性代数

- 在深度学习中, 我们也使用一些不那么常规的符号.
  我们允许矩阵和向量相加, 产生另一个矩阵:
  - $$ C = A + b $$,
    其中
    $$ C_{i, j} = A_{i, j} + b_{j} $$.
  - 换言之, 向量 `b` 和矩阵 `A` 的每一行相加.
- 这个简写方法使我们无须在加法操作前定义一个将向量 `b` 复制到每一行而生成的矩阵.
  - 这种隐式地复制向量 `b` 到很多位置的方式, 称为`广播`.

---

- 有时我们需要衡量一个向量的大小.
  - 在机器学习中, 我们经常使用称为`范数` (`norm`) 的函数来衡量向量大小.
  - 形式上,
    $$ L^p $$
    范数定义如下
  - $$ \| x \| _{p} = (\sum_{i} \mid x_i \mid ^{p})^{\frac{1}{p}} $$
  - 其中
    $$ p \in \mathbb{R} $$,
    $$ p \ge 1 $$.
- 范数 (包括
  $$ L^p $$
  范数) 是将向量映射到非负值的函数.
  - 直观上来说, 向量 `x` 的范数衡量从原点到点 `x` 的距离.
  - 更严格地说, 范数是满足下列性质的任意函数:
  - $$ f(x) = 0 \Longrightarrow x = 0 $$;
  - $$ f(x + y) \le f(x) + f(y) $$
    (__三角不等式__);
  - $$ \forall a \in \mathbb{R}, f(ax) = \mid a \mid f(x) $$.
- 当 `p = 2` 时,
  $$ L^2 $$
  范数称为欧几里得范数.
  - 它表示从原点出发到向量 `x` 确定的点的欧几里得距离.
  - $$ L^2 $$
    范数在机器学习中出现得十分频繁, 经常简化表示为
    $$ \| x \| $$,
    略去了下标 `2`.
  - 平方
    $$ L^2 $$
    范数也经常用来衡量向量的大小, 可以简单地通过点积
    $$ x^{\top} x $$
    计算.
- 平方
  $$ L^2 $$
  范数在数学和计算上都比
  $$ L^2 $$
  范数本身更方便.
  - 例如, 平方
    $$ L^2 $$
    范数对 `x` 中每个元素的导数只取决于对应的元素, 而
    $$ L^2 $$
    范数对每个元素的导数和整个向量相关.
  - 但是在很多情况下, 平方
    $$ L^2 $$
    范数也可能不受欢迎, 因为它在原点附近增长得十分缓慢.
  - 在某些机器学习应用中, 区分恰好是零的元素和非零但值很小的元素是很重要的.
  - 在这些情况下, 我们转而使用在各个位置斜率相同,
    同时保持简单的数学形式的函数:
    $$ L^1 $$
    范数.
  - $$ L^1 $$
    范数可以简化如下
  - $$ \| x \|_{1} = \sum_{i} \mid x_{i} \mid $$

---

- 另外一个经常在机器学习中出现的范数是
  $$ L^{\infty} $$
  范数, 也被称为`最大范数` (max norm).
  - 这个范数表示向量中具有最大幅值的元素的绝对值:
  - $$ \| x \|_{\infty} = \underset{i}{max} \mid x_i \mid $$
- 有时候我们可能也希望衡量矩阵的大小. 在深度学习中,
  最常见的做法是使用 `Frobenius 范数` (Frobenius norm), 即
  - $$ \| A \|_{F} = \sqrt{\sum_{i, j} A_{i, j}^2} $$
  - 其类似于向量的
    $$ L^2 $$
    范数.
- 两个向量的点积可以用范数来表示, 具体如下
  - $$ {x}^{\top}{y} = \| x \|_{2} \| y \|_{2} \cos \theta $$
  - 其中
    $$ \theta $$
    表示 `x` 和 `y` 之间的夹角.

---

- 方阵 `A` 的特征向量是指与 `A` 相乘后相当于对该向量进行缩放的非零向量 `v`:
  - $$ \mathbf{A} \mathbf{v} = \lambda \mathbf{v} $$
  - 其中标量
    $$ \lambda $$
    称为这个特征向量对应的特征值.
  - (类似地, 我们也可以定义左特征向量
    $$ \mathbf{v}^{\top} \mathbf{A} = \lambda \mathbf{v}^{\top} $$,
    但是通常我们更关注右特征向量).
- 如果 `v` 是 `A` 的特征向量, 那么任何缩放后的向量
  $$ s \mathbf{v} (s \in \mathbb{R}, s \ne 0) $$
  也是 `A` 的特征向量.
  - 此外, `sv` 和 `v` 有相同的特征值.
  - 基于这个原因, 通常我们只考虑单位特征向量.
- 假设矩阵 `A` 有 `n` 个线性无关的特征向量
  $$ \{ \mathbf{v}^{(1)}, ..., \mathbf{v}^{(n)} \} $$,
  对应着特征值
  $$ \{ \lambda_{1}, ..., \lambda_{n} \} $$.
  - 我们将特征向量连接成一个矩阵, 使得每一列是一个特征向量:
    $$ \mathbf{V} = \left [ \mathbf{v}^{(1)}, ..., \mathbf{v}^{(n)} \right ] $$.
  - 类似地, 我们也可以将特征值连接成一个向量
    $$ \lambda = \left [ \lambda_{1}, ..., \lambda_{n} \right ] ^{\top} $$.
  - 因此 `A` 的特征分解可以记作
    $$ \mathbf{A} = \mathbf{V} diag(\lambda) \mathbf{V}^{-1} $$

---

- 我们已经看到了构建具有特定特征值和特征向量的矩阵,
  能够使我们在目标方向上延伸空间. 然而,
  我们也常常希望将矩阵分解成特征值和特征向量.
  - 这样可以帮助我们分析矩阵的特定性质,
    就像质因数分解有助于我们理解整数.
- 不是每一个矩阵都可以分解成特征值和特征向量.
  在某些情况下, 特征分解存在, 但是会涉及复数而非实数.
  - 幸运的是, 在本书中, 我们通常只需要分解一类有简单分解的矩阵.
- 具体来讲, 每个实对称矩阵都可以分解成实特征向量和实特征值:
  - $$ \mathbf{A} = \mathbf{Q} \mathbf{Λ} \mathbf{Q} ^{\top} $$
  - 其中 `Q` 是 `A` 的特征向量组成的正交矩阵, `Λ` 是对角矩阵.
  - 特征值
    $$ Λ_{i, i} $$
    对应的特征向量是矩阵 `Q` 的第 `i` 列, 记作
    $$ Q_{:, i} $$.
  - 因为 `Q` 是正交矩阵, 我们可以将 `A` 看作沿方向
    $$ v^{(i)} $$
    延展
    $$ \lambda_{i} $$
    倍的空间.

---

- 虽然任意一个实对称矩阵 `A` 都有特征分解, 但是特征分解可能并不唯一.
  - 如果两个或多个特征向量拥有相同的特征值,
    那么在由这些特征向量产生的生成子空间中,
    任意一组正交向量都是该特征值对应的特征向量.
- 因此, 我们可以等价地从这些特征向量中构成 `Q` 作为替代.
  - 按照惯例, 我们通常按降序排列 `Λ` 的元素.
  - 在该约定下, 特征分解唯一, 当且仅当所有的特征值都是唯一的.
  - 矩阵的特征分解给了我们很多关于矩阵的有用信息.

---

- 矩阵是奇异的, 当且仅当含有零特征值.
  实对称矩阵的特征分解也可以用于优化二次方程
  $$ f(x) = x^{\top} \mathbf{A} x $$,
  其中限制
  $$ \| x \|_{2} = 1 $$.
  - 当 `x` 等于 `A` 的某个特征向量时, `f` 将返回对应的特征值.
  - 在限制条件下, 函数 `f` 的最大值是最大特征值, 最小值是最小特征值.
- 所有特征值都是正数的矩阵称为`正定`;
  所有特征值都是非负数的矩阵称为`半正定`.
  - 同样地, 所有特征值都是负数的矩阵称为`负定`;
    所有特征值都是非正数的矩阵称为`半负定`.
- 半正定矩阵受到关注是因为它们保证
  $$ \forall x, x^{\top} \mathbf{A} x ≥ 0 $$.
  - 此外, 正定矩阵还保证
    $$ x^{\top} \mathbf{A} x = 0 \Rightarrow x = 0 $$.

---

- 奇异值分解 (singular value decomposition, SVD),
  是将矩阵分解为奇异向量 (singular vector) 和奇异值 (singular value).
  - 通过奇异值分解, 我们会得到一些与特征分解相同类型的信息.
  - 然而, 奇异值分解有更广泛的应用.
  - 每个实数矩阵都有一个奇异值分解, 但不一定都有特征分解.
  - 例如, 非方阵的矩阵没有特征分解, 这时我们只能使用奇异值分解.
- 回想一下, 我们使用特征分解去分析矩阵 `A` 时,
  得到特征向量构成的矩阵 `V` 和特征值构成的向量
  $$ \lambda $$,
  我们可以重新将 `A` 写作
  - $$ \mathbf{A} = \mathbf{V} diag(\lambda) \mathbf{V}^{-1} $$
- 奇异值分解是类似的, 只不过这回我们将矩阵 `A` 分解成三个矩阵的乘积:
  - $$ \mathbf{A} = \mathbf{U} \mathbf{D} \mathbf{V} ^{\top} $$
  - 假设 `A` 是一个
    $$ m \times n $$
    的矩阵, 那么 `U` 是一个
    $$ m \times m $$
    的矩阵, `D` 是一个
    $$ m \times n $$
    的矩阵, `V` 是一个
    $$ n \times n $$
    矩阵.
- 这些矩阵中的每一个经定义后都拥有特殊的结构.
  矩阵 `U` 和 `V` 都定义为正交矩阵,
  而矩阵 `D` 定义为对角矩阵.
  - 注意, 矩阵 `D` 不一定是方阵.
- 对角矩阵 `D` 对角线上的元素称为矩阵 `A` 的奇异值 (singular value).
  - 矩阵 `U` 的列向量称为左奇异向量 (left singular vector),
    矩阵 `V` 的列向量称右奇异向量 (right singular vector).
- 事实上, 我们可以用与 `A` 相关的特征分解去解释 `A` 的奇异值分解.
  - `A` 的左奇异向量 (left singular vector) 是
    $$ \mathbf{A} \mathbf{A} ^{\top} $$
    的特征向量.
  - `A` 的右奇异向量 (right singular vector) 是
    $$ \mathbf{A} ^{\top} \mathbf{A} $$
    的特征向量.
  - `A` 的非零奇异值是
    $$ \mathbf{A} \mathbf{A} ^{\top} $$
    特征值的平方根, 同时也是
    $$ \mathbf{A} ^{\top} \mathbf{A} $$
    特征值的平方根.

> `SVD` 最有用的一个性质可能是拓展矩阵求逆到非方矩阵上.

---

- 对于非方矩阵而言, 其逆矩阵没有定义.
  假设在下面的问题中, 我们希望通过矩阵 `A` 的左逆
  `B` 来求解线性方程:
  - $$ \mathbf{A} x = y $$
- 等式两边左乘左逆 `B` 后, 我们得到
  - $$ x = \mathbf{B} y $$
- 取决于问题的形式, 我们可能无法设计一个唯一的映射将 `A` 映射到 `B`.
  - 如果矩阵 `A` 的行数大于列数, 那么上述方程可能没有解.
  - 如果矩阵 `A` 的行数小于列数, 那么上述矩阵可能有多个解.

---

- __伪逆__ 使我们在这类问题上取得了一定的进展. 矩阵 `A` 的伪逆定义为

$$
\mathbf{A}^{+} = \lim_{a \searrow 0}
(\mathbf{A}^{\top} \mathbf{A} + a \mathbf{I})^{-1}
\mathbf{A}^{\top}
$$

- 计算伪逆的实际算法没有基于这个定义, 而是使用下面的公式
  - $$ \mathbf{A}^{+} = \mathbf{V} \mathbf{D}^{+} \mathbf{U}^{\top} $$
  - 其中, 矩阵 `U`, `D` 和 `V` 是矩阵 `A` 奇异值分解后得到的矩阵.
  - 对角矩阵 `D` 的伪逆
    $$ D^{+} $$
    是其非零元素取倒数之后再转置得到的.
- 当矩阵 `A` 的列数多于行数时, 使用伪逆求解线性方程是众多可能解法中的一种.
  - 特别地,
    $$ x = \mathbf{A}^{+} y $$
    是方程所有可行解中欧几里得范数
    $$ \| x \|_{2} $$
    最小的一个.
- 当矩阵 `A` 的行数多于列数时, 可能没有解. 在这种情况下, 通过伪逆得到的
  `x` 使得 `Ax` 和 `y` 的欧几里得距离
  $$ \| \mathbf{A} x - y \|_{2} $$
  最小.

---

- 迹运算返回的是矩阵对角元素的和:
  - $$ Tr(A) = \sum_{i} A_{i, i} $$.
- 迹运算提供了另一种描述矩阵 Frobenius 范数的方式:
  - $$ \| A \|_{F} = \sqrt{Tr(A A^{\top})} $$
- 多个矩阵相乘得到的方阵的迹,
  和将这些矩阵中的最后一个挪到最前面之后相乘的迹是相同的.
  - 当然, 我们需要考虑挪动之后矩阵乘积依然定义良好:
  - $$ Tr(ABC) = Tr(CAB) = Tr(BCA) $$
- 或者更一般地,

$$
Tr(\prod_{i = 1}^{n} F^{(i)}) =
Tr(F^{(n)} \prod_{i = 1}^{n - 1} F^{(i)})
$$

- 即使循环置换后矩阵乘积得到的矩阵形状变了,
  迹运算的结果依然不变.

## 概率与信息论

- 期望是线性的, 例如,

$$
\mathbb{E}_{x} \left [ α f(x) + β g(x) \right ] =
α \mathbb{E}_{x} \left [ f(x) \right ] +
β \mathbb{E}_{x} \left [ g(x) \right ]
$$

- 其中 `α` 和 `β` 不依赖于 `x`.

---

- `方差`衡量的是当我们对 `x` 依据它的概率分布进行采样时,
  随机变量 `x` 的函数值会呈现多大的差异:

$$
Var(f(x)) =
\mathbb{E} \left [ (
  f(x) - \mathbb{E} \left [ f(x) \right ]
)^{2} \right ]
$$

- 当方差很小时, `f(x)` 的值形成的簇比较接近它们的期望值.
  - 方差的平方根被称为`标准差`.

---

- `协方差`在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度:

$$
Cov(f(x), g(y)) = \mathbb{E} \left [
  (f(x) - \mathbb{E} \left [ f(x) \right ])
  (g(y) - \mathbb{E} \left [ g(y) \right ])
\right ]
$$

- 协方差的绝对值如果很大, 则意味着变量值变化很大,
  并且它们同时距离各自的均值很远.
  - 如果协方差是正的, 那么两个变量都倾向于同时取得相对较大的值.
  - 如果协方差是负的, 那么其中一个变量倾向于取得相对较大的值的同时,
    另一个变量倾向于取得相对较小的值, 反之亦然.
- 其他的衡量指标如`相关系数`将每个变量的贡献归一化,
  为了只衡量变量的相关性而不受各个变量尺度大小的影响.
- 协方差和相关性是有联系的, 但实际上是不同的概念.
  - 它们是有联系的:
  - 如果两个变量相互独立, 那么它们的协方差为零;
  - 如果两个变量的协方差不为零, 那么它们一定是相关的.
- 然而, 独立性又是和协方差完全不同的性质.
  - 两个变量如果协方差为零, 它们之间一定没有线性关系.
  - 独立性是比零协方差的要求更强, 因为独立性还排除了非线性的关系.
- 两个变量相互依赖, 但是具有零协方差是可能的.
  - 例如, 假设我们首先从区间 `[-1, 1]` 上的均匀分布中采样出一个实数 `x`,
    然后对一个随机变量 `s` 进行采样.
  - `s` 以
    $$ \frac{1}{2} $$
    的概率值为 `1`, 否则为 `-1`.
  - 我们可以通过令 `y = sx` 来生成一个随机变量 `y`.
    显然, `x` 和 `y` 不是相互独立的,
    因为 `x` 完全决定了 `y` 的尺度.
  - 然而, `Cov(x, y) = 0`.
- 随机向量
  $$ x \in \mathbb{R}^{n} $$
  的`协方差矩阵`是一个
  $$ n \times n $$
  的矩阵, 并且满足
  - $$ Cov(x)_{i, j} = Cov(x_i, x_j) $$
  - 协方差矩阵的对角元是方差:
  - $$ Cov(x_i, x_i) = Var(x_i) $$

## 深度前馈网络

```
量化模型的容量使得统计学习理论可以进行量化预测.
统计学习理论中最重要的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量增长而增长,
但随着训练样本增多而下降. 这些边界为机器学习算法可以有效解决问题提供了理论验证,
但是它们很少应用于实际中的深度学习算法. 一部分原因是边界太松,
另一部分原因是很难确定深度学习算法的容量. 由于有效容量受限于优化算法的能力,
确定深度学习模型容量的问题特别困难. 而且对于深度学习中的一般非凸优化问题,
我们只有很少的理论分析.

我们必须记住虽然更简单的函数更可能泛化 (训练误差和测试误差的差距小),
但我们仍然需要选择一个充分复杂的假设以达到低的训练误差.
通常, 当模型容量上升时, 训练误差会下降, 直到其渐近最小可能误差 (假设误差度量有最小值).
通常, 泛化误差是一个关于模型容量的 U 形曲线函数.

为考虑容量任意高的极端情况, 我们介绍非参数模型的概念. 至此, 我们只探讨过参数模型,
例如线性回归. 参数模型学习的函数在观测到新数据前, 参数向量的分量个数是有限且固定的.
非参数模型没有这些限制.

有时, 非参数模型仅是一些不能实际实现的理论抽象 (比如搜索所有可能概率分布的算法).
然而, 我们也可以设计一些实用的非参数模型, 使它们的复杂度和训练集大小有关.
```

```
有时一个选项被设为学习算法不用学习的超参数, 是因为它太难优化了.
更多的情况是, 该选项必须是超参数, 因为它不适合在训练集上学习.
这适用于控制模型容量的所有超参数. 如果在训练集上学习超参数,
这些超参数总是趋向于最大可能的模型容量, 导致过拟合.
例如, 相比低次多项式和正的权重衰减设定,
更高次的多项式和权重衰减参数设定 λ = 0 总能在训练集上更好地拟合.

为了解决这个问题, 我们需要一个训练算法观测不到的验证集样本.
早先我们讨论过和训练数据相同分布的样本组成的测试集,
它可以用来估计学习过程完成之后的学习器的泛化误差.
其重点在于测试样本不能以任何形式参与到模型的选择中, 包括设定超参数.
基于这个原因, 测试集中的样本不能用于验证集.
因此, 我们总是从训练数据中构建验证集.
特别地, 我们将训练数据分成两个不相交的子集.
其中一个用于学习参数. 另一个作为验证集,
用于估计训练中或训练后的泛化误差, 更新超参数.
用于学习参数的数据子集通常仍被称为训练集,
尽管这会和整个训练过程用到的更大的数据集相混.
用于挑选超参数的数据子集被称为验证集.
通常, 80% 的训练数据用于训练, 20% 用于验证.
由于验证集是用来"训练"超参数的, 尽管验证集的误差通常会比训练集误差小,
验证集会低估泛化误差. 所有超参数优化完成之后, 泛化误差可能会通过测试集来估计.
```

- 随机梯度下降的核心是, 梯度是期望. 期望可使用小规模的样本近似估计.
  - 具体而言, 在算法的每一步,
    我们从训练集中均匀抽出一小批量 (minibatch) 样本
  - $$ \mathbb{B} = \left \{ x^{(1)}, ..., x^{(m')} \right \} $$.
  - 小批量的数目 `m'` 通常是一个相对较小的数, 从一到几百.
  - 重要的是, 当训练集大小 `m` 增长时, `m'` 通常是固定的.
  - 我们可能在拟合几十亿的样本时, 每次更新计算只用到几百个样本.

```
尽管术语"流形"有正式的数学定义, 但是机器学习倾向于更松散地定义一组点,
只需要考虑少数嵌入在高维空间中的自由度或维数就能很好地近似.
每一维都对应着局部的变化方向.
在机器学习中, 我们允许流形的维数从一个点到另一个点有所变化.
这经常发生于流形和自身相交的情况中.
例如, 数字"8"形状的流形在大多数位置只有一维, 但在中心的相交处有两维.
```

- 具体来说, 万能近似定理表明,
  一个前馈神经网络如果具有线性输出层和至少一层具有任何一种"挤压"性质的激活函数
  (例如 logistic sigmoid 激活函数) 的隐藏层, 只要给予网络足够数量的隐藏单元,
  它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的 Borel 可测函数.
  前馈网络的导数也可以任意好地来近似函数的导数.
  - Borel 可测的概念超出了本书的范畴.

---

- 总之, 具有单层的前馈网络足以表示任何函数, 但是网络层可能大得不可实现,
  并且可能无法正确地学习和泛化.
  - 在很多情况下, 使用更深的模型能够减少表示期望函数所需的单元的数量,
    并且可以减少泛化误差.
- 存在一些函数族能够在网络的深度大于某个值 `d` 时被高效地近似,
  而当深度被限制到小于或等于 `d` 时需要一个远远大于之前的模型.
  - 在很多情况下, 浅层模型所需的隐藏单元的数量是 `n` 的指数级.

---

- 反向传播这个术语经常被误解为用于多层神经网络的整个学习算法.
  实际上, 反向传播仅指用于计算梯度的方法, 而另一种算法,
  例如随机梯度下降, 使用该梯度来进行学习.
  - 此外, 反向传播经常被误解为仅适用于多层神经网络,
    但是原则上它可以计算任何函数的导数
    (对于一些函数, 正确的响应是报告函数的导数是未定义的).

## 深度学习中的正则化

```
深度学习界在某种程度上已经与更广泛的计算机科学界隔离开来,
并且在很大程度上发展了自己关于如何进行微分的文化态度.
一般来说, 自动微分 (automatic differentiation)
领域关心如何以算法方式计算导数.
这里描述的反向传播算法只是自动微分的一种方法.
它是一种称为反向模式累加 (reverse mode accumulation)
的更广泛类型的技术的特殊情况. 其他方法以不同的顺序来计算链式法则的子表达式.
一般来说, 确定一种计算的顺序使得计算开销最小, 是困难的问题.
找到计算梯度的最优操作序列是 NP 完全问题, 在这种意义上,
它可能需要将代数表达式简化为它们最廉价的形式.
```

```
其中一个算法上的变化是用交叉熵族损失函数替代均方误差损失函数.
均方误差在 20 世纪 80 年代和 90 年代流行, 但逐渐被交叉熵损失替代,
并且最大似然原理的想法在统计学界和机器学习界之间广泛传播.
使用交叉熵损失大大提高了具有 sigmoid 和 softmax 输出的模型的性能,
而当使用均方误差损失时会存在饱和和学习缓慢的问题.
```

```
Dropout 训练与 Bagging 训练不太一样.
在 Bagging 的情况下, 所有模型都是独立的.
在 Dropout 的情况下, 所有模型共享参数,
其中每个模型继承父神经网络参数的不同子集.
参数共享使得在有限可用的内存下表示指数级数量的模型变得可能.
在 Bagging 的情况下, 每一个模型在其相应训练集上训练到收敛.
在 Dropout 的情况下, 通常大部分模型都没有显式地被训练,
因为通常父神经网络会很大, 以至于到宇宙毁灭都不可能采样完所有的子网络.
取而代之的是, 在单个步骤中我们训练一小部分的子网络,
参数共享会使得剩余的子网络也能有好的参数设定. 这些是仅有的区别.
除了这些, Dropout 与 Bagging 算法一样.
例如, 每个子网络中遇到的训练集确实是有放回采样的原始训练集的一个子集.
```

## 深度模型中的优化

```
另一个促使我们从小数目样本中获得梯度的统计估计的动机是训练集的冗余.
在最坏的情况下, 训练集中所有的 m 个样本都是彼此相同的拷贝.
基于采样的梯度估计可以使用单个样本计算出正确的梯度,
而比原来的做法少花了 m 倍时间. 实践中, 我们不太可能真的遇到这种最坏情况,
但可能会发现大量样本都对梯度做出了非常相似的贡献.
```

```
更大的初始权重具有更强的破坏对称性的作用, 有助于避免冗余的单元.
它们也有助于避免在每层线性成分的前向或反向传播中丢失信号 --
矩阵中更大的值在矩阵乘法中有更大的输出.
如果初始权重太大, 那么会在前向传播或反向传播中产生爆炸的值.
在循环网络中, 很大的权重也可能导致混沌 (chaos)
(对于输入中很小的扰动非常敏感, 导致确定性前向传播过程表现随机).
在一定程度上, 梯度爆炸问题可以通过梯度截断来缓解
(执行梯度下降步骤之前设置梯度的阈值).
较大的权重也会产生使得激活函数饱和的值, 导致饱和单元的梯度完全丢失.
这些竞争因素决定了权重的理想初始大小.
```

------------------

- [深度强化学习图解](https://book.douban.com/subject/36019621/)

> 不推荐! 原书一般, 翻译较差~
