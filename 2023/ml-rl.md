```
                        三种 TD 算法的对比

学习类型            选择        求值    自举造成的偏差    最大化造成的高估
原始 Q 学习         DQN        DQN        严重            严重
Q 学习 + 目标网络  目标网络     目标网络     不严重          严重
双 Q 学习           DQN       目标网络     不严重         不严重
```

- 在训练的时候往 `DQN` 的参数中加入噪声, 不仅有利于探索, 还能增强健壮性.
  健壮性的意思是即使参数被扰动, `DQN` 也能对动作价值
  $$ Q_{*} $$
  做出可靠的估计.
  - 为什么噪声可以让 `DQN` 有更强的健壮性呢?
- 假设在训练过程中不加入噪声, 把学出的参数记作
  $$ μ $$.
  - 当参数严格等于
    $$ μ $$
    的时候, `DQN` 可以对最优动作价值做出较为准确的估计.
  - 但是对
    $$ μ $$
    做较小的扰动, 就可能会让 `DQN` 的输出偏离很远,
    所谓 "失之毫厘, 谬以千里".
- 噪声 `DQN` 训练的过程中, 参数带有噪声:
  $$ w = μ + σ \circ ξ $$.
  - 训练迫使 `DQN` 在参数带噪声的情况下最小化 `TD` 误差,
    也就是迫使 `DQN` 容忍对参数的扰动.
  - 训练出的 `DQN` 具有健壮性: 参数不严格等于
    $$ μ $$
    也没关系, 只要参数在
    $$ μ $$
    的邻域内, `DQN` 做出的预测都应该比较合理.
  - 用噪声 `DQN`, 不会出现 "失之毫厘, 谬以千里".

---

- 实际编程实现 `DQN` 的时候, 需要全部用上本章介绍的 `4` 种技巧:
  - 优先经验回放,
  - 双 `Q` 学习,
  - 对决网络,
  - 噪声 `DQN`.
- 应该用`对决网络`的神经网络结构, 而不是简单的 `DQN` 结构.
- 往对决网络的参数 `w` 中加入噪声, 得到噪声 `DQN`, 记作
  $$ \overset{\sim}{Q} (s, a, ξ; μ, σ) $$.
- 训练要用双 `Q` 学习, 优先经验回放, 而不是原始的 `Q` 学习.
  - 双 `Q` 学习需要目标网络
    $$ \overset{\sim}{Q} (s, a, ξ; μ^{-}, σ^{-}) $$
    计算 `TD` 目标.
  - 它跟噪声 `DQN` 的结构相同, 但是参数不同.
