- [深度强化学习](https://book.douban.com/subject/36161659/)

---

- 随机性有两个来源: 动作和状态.
  动作的随机性来源于策略, 状态的随机性来源于状态转移.
  策略由策略函数决定, 状态转移由状态转移函数决定.
  - 本书中用
    $$ S_t $$
    和
    $$ s_t $$
    分别表示 `t` 时刻的状态及其观测值, 用
    $$ A_t $$
    和
    $$ a_t $$
    分别表示 `t` 时刻的动作及其观测值.

- `t` 时刻的动作价值函数
  $$ Q_{\pi} (s_t, a_t) $$
  依赖于以下三个因素.
  - 第一, 当前状态
    $$ s_t $$.
    当前状态越好,
    $$ Q_{\pi} (s_t, a_t) $$
    越大, 也就是说回报的期望值越大.
  - 第二, 当前动作
    $$ a_t $$.
    智能体执行的动作越好,
    $$ Q_{\pi} (s_t, a_t) $$
    越大.
  - 第三, 策略函数
    $$ \pi $$.
    策略决定未来的动作
    $$ A_{t+1} $$,
    $$ A_{t+2} $$,
    ...,
    $$ A_n $$
    的好坏: 策略越好,
    $$ Q_{\pi} (s_t, a_t) $$
    越大.

- 注意, 算法所需数据为四元组
  $$ (s_t, a_t, r_t, s_{t+1}) $$,
  与控制智能体运动的策略无关. 这就意味着可以用任何策略控制智能体与环境交互,
  同时记录下算法运动轨迹, 作为训练数据.
  - 因此, `DQN` 的训练可以分割成两个独立的部分:
  - 收集训练数据和更新参数 `w`.

- 让行为策略带有随机性的好处是能探索更多没见过的状态. 在实验中, 初始让
  $$ \epsilon $$
  比较大 (比如
  $$ \epsilon = 0.5 $$
  ); 在训练过程中, 让
  $$ \epsilon $$
  逐渐衰减, 在几十万步之后衰减到较小的值 (比如
  $$ \epsilon = 0.01 $$
  ), 此后固定住
  $$ \epsilon = 0.01 $$.

---

- 异策略的好处是可以用行为策略收集经验, 把
  $$ (s_t, a_t, r_t, s_{t+1}) $$
  这样的四元组记录到一个缓存里, 事后反复利用这些经验去更新目标策略.
  - 这个缓存称为`经验回放缓存`, 而这种将智能体与环境交互的记录暂时保存,
    然后从中采样和学习的训练方式称为`经验回放`.
  - 注意, 经验回放只适用于异策略, 不适用于同策略,
    其原因是收集经验时用的行为策略不同于想要训练出的目标策略.

- 经验回放的一个好处是打破了序列的相关性.
  训练 `DQN` 的时候, 每次我们用一个四元组对 `DQN` 的参数做一次更新,
  我们希望相邻两次使用的四元组是独立的.
- 然而当智能体收集经验的时候, 相邻两个四元组
  $$ (s_t, a_t, r_t, s_{t+1}) $$
  和
  $$ (s_{t+1}, a_{t+1}, r_{t+1}, s_{t+2}) $$
  有很强的相关性.
  - 依次使用这些强关联的四元组训练 `DQN`, 效果往往会很差.
- 而经验回放每次从缓存里随机抽取一个四元组, 用来对 `DQN` 参数做一次更新.
  这样随机抽到的四元组相互之间是独立的, 消除了相关性.

---

- 读者可能会提出下面的问题. 如果样本
  $$ (s_j, a_j, r_j, s_{j+1}) $$
  很重要, 它被抽到的概率
  $$ p_j $$
  很大, 可是它的学习率却很小.
  - 当
    $$ \beta = 1 $$
    时, 如果抽样概率
    $$ p_j $$
    变大 `10` 倍, 则学习率
    $$ a_j $$
    减小为原来的 `1/10`.
  - 抽样概率, 学习率两者岂不是抵消了, 那么优先经验回放有什么意义呢?
  - 大抽样概率, 小学习率两者其实并没有抵消, 因为下面两种方式并不等价:
  - 设置学习率为 `a`, 使用样本
    $$ (s_j, a_j, r_j, s_{j+1}) $$
    计算一次梯度, 更新一次参数 `w`;
  - 设置学习率为
    $$ \frac{a}{10} $$,
    使用样本
    $$ (s_j, a_j, r_j, s_{j+1}) $$
    计算十次梯度, 更新十次参数 `w`.
- 乍看起来两种方式区别不大, 但其实第二种方式对样本的利用更高效,
  它的缺点是计算量大了 `10` 倍, 所以只用于重要的样本.

---

- `Q` 学习算法有一个缺陷: 用它训练出来的 `DQN` 会高估真实的价值,
  而且高估通常是非均匀的. 这个缺陷导致 `DQN` 的表现很差.
  高估问题并不是 `DQN` 模型的缺陷, 而是 `Q` 学习算法的缺陷.
- `Q` 学习产生高估的原因有两个:
  - 第一, 自举导致偏差传播;
  - 第二, 最大化导致 `TD` 目标高估真实价值.
- 为了缓解高估, 需要从上述两个原因下手, 改进 `Q` 学习算法.
  - 双 `Q` 学习算法是一种有效的改进, 可以大幅缓解高估及其危害.

- 找到了高估产生的原因, 就可以想办法解决问题.
  想要避免 `DQN` 的高估, 要么切断自举, 要么避免最大化.
  - 注意, 高估并不是 `DQN` 自身的属性, 而纯粹是算法造成的.
    想要避免高估, 就要用更好的算法替代原始的 `Q` 学习算法.


```
                        三种 TD 算法的对比

学习类型            选择        求值    自举造成的偏差    最大化造成的高估
原始 Q 学习         DQN        DQN        严重            严重
Q 学习 + 目标网络  目标网络     目标网络     不严重          严重
双 Q 学习           DQN       目标网络     不严重         不严重
```

- 在训练的时候往 `DQN` 的参数中加入噪声, 不仅有利于探索, 还能增强健壮性.
  健壮性的意思是即使参数被扰动, `DQN` 也能对动作价值
  $$ Q_{*} $$
  做出可靠的估计.
  - 为什么噪声可以让 `DQN` 有更强的健壮性呢?
- 假设在训练过程中不加入噪声, 把学出的参数记作
  $$ μ $$.
  - 当参数严格等于
    $$ μ $$
    的时候, `DQN` 可以对最优动作价值做出较为准确的估计.
  - 但是对
    $$ μ $$
    做较小的扰动, 就可能会让 `DQN` 的输出偏离很远,
    所谓 "失之毫厘, 谬以千里".
- 噪声 `DQN` 训练的过程中, 参数带有噪声:
  $$ w = μ + σ \circ ξ $$.
  - 训练迫使 `DQN` 在参数带噪声的情况下最小化 `TD` 误差,
    也就是迫使 `DQN` 容忍对参数的扰动.
  - 训练出的 `DQN` 具有健壮性: 参数不严格等于
    $$ μ $$
    也没关系, 只要参数在
    $$ μ $$
    的邻域内, `DQN` 做出的预测都应该比较合理.
  - 用噪声 `DQN`, 不会出现 "失之毫厘, 谬以千里".

---

- 实际编程实现 `DQN` 的时候, 需要全部用上本章介绍的 `4` 种技巧:
  - 优先经验回放,
  - 双 `Q` 学习,
  - 对决网络,
  - 噪声 `DQN`.
- 应该用`对决网络`的神经网络结构, 而不是简单的 `DQN` 结构.
- 往对决网络的参数 `w` 中加入噪声, 得到噪声 `DQN`, 记作
  $$ \overset{\sim}{Q} (s, a, ξ; μ, σ) $$.
- 训练要用双 `Q` 学习, 优先经验回放, 而不是原始的 `Q` 学习.
  - 双 `Q` 学习需要目标网络
    $$ \overset{\sim}{Q} (s, a, ξ; μ^{-}, σ^{-}) $$
    计算 `TD` 目标.
  - 它跟噪声 `DQN` 的结构相同, 但是参数不同.

---

- [贝尔曼方程](https://en.wikipedia.org/wiki/Bellman_equation)
