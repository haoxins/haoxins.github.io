## 卷积网络

```
卷积是一种特殊的线性运算.
卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络.
```

> ML 中的卷积运算和数学意义上的卷积有差异~

```
卷积运算可交换性的出现是因为我们将核相对输入进行了翻转,
从 m 增大的角度来看, 输入的索引在增大, 但是核的索引在减小.
我们将核翻转的唯一目的是实现可交换性. 尽管可交换性在证明时很有用,
但在神经网络的应用中却不是一个重要的性质. 与之不同的是,
许多神经网络库会实现一个相关的函数, 称为互相关函数,
和卷积运算几乎一样但是并没有对核进行翻转.

许多机器学习的库实现的是互相关函数但是称之为卷积.
在这本书中我们遵循把两种运算都叫作卷积的这个传统,
在与核翻转有关的上下文中, 我们会特别指明是否对核进行了翻转.
```

```
在机器学习中, 学习算法会在核合适的位置学得恰当的值,
所以一个基于核翻转的卷积运算的学习算法所学得的核,
是对未进行翻转的算法学得的核的翻转.
单独使用卷积运算在机器学习中是很少见的, 卷积经常与其他的函数一起使用,
无论卷积运算是否对它的核进行了翻转, 这些函数的组合通常是不可交换的.
```

```
处于卷积网络更深的层中的单元, 它们的接受域要比处在浅层的单元的接受域更大.
如果网络还包含类似步幅卷积或者池化之类的结构特征, 这种效应会加强.
这意味着在卷积网络中尽管直接连接都是很稀疏的,
但处在更深的层中的单元可以间接地连接到全部或者大部分输入图像.
```

```
池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出.
例如, 最大池化函数给出相邻矩形区域内的最大值.
其他常用的池化函数包括相邻矩形区域内的平均值,
L2 范数以及基于距中心像素距离的加权平均函数.
```

```
使用池化可以看作增加了一个无限强的先验:
这一层学得的函数必须具有对少量平移的不变性.
当这个假设成立时, 池化可以极大地提高网络的统计效率.
对空间区域进行池化产生了平移不变性,
但当我们对分离参数的卷积的输出进行池化时,
特征能够学得应该对于哪种变换具有不变性.
```

```
我们可以把卷积网络类比成全连接网络, 但对于这个全连接网络的权重有一个无限强的先验.
这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同, 但可以在空间上移动.
这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重以外,
其余的权重都为零.
总之, 我们可以把卷积的使用当作对网络中一层的参数引入了一个无限强的先验概率分布.
这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性.
类似地, 使用池化也是一个无限强的先验: 每一个单元都具有对少量平移的不变性.
```

```
当我们知道每一个特征都是一小块空间的函数并且相同的特征不会出现在所有的空间上时,
局部连接层是很有用的.
```

```
平铺卷积对卷积层和局部连接层进行了折衷.
这里并不是对每一个空间位置的权重集合进行学习,
我们学习一组核使得当我们在空间移动时它们可以循环利用.
这意味着在近邻的位置上拥有不同的过滤器, 就像局部连接层一样,
但是对于这些参数的存储需求仅仅会增长常数倍,
这个常数就是核的集合的大小, 而不是整个输出的特征映射的大小.
```

- 卷积等效于使用傅里叶变换将输入与核都转换到频域, 执行两个信号的逐点相乘,
  再使用傅里叶逆变换转换回时域. 对于某些问题的规模,
  这种算法可能比离散卷积的朴素实现更快.
- 当一个 `d` 维的核可以表示成 `d` 个向量 (每一维一个向量) 的外积时,
  该核被称为可分离的. 当核可分离时, 朴素的卷积是低效的.
  它等价于组合 `d` 个一维卷积, 每个卷积使用这些向量中的一个.
  - 组合方法显著快于使用它们的外积来执行一个 `d` 维的卷积,
    并且核也只要更少的参数来表示成向量.
  - 如果核在每一维都是 `w` 个元素宽, 那么朴素的多维卷积需要
    $$ O(w^d) $$
    的运行时间和参数存储空间, 而可分离卷积只需要
    $$ O(w \times d) $$
    的运行时间和参数存储空间.
  - 当然, 并不是每个卷积都可以表示成这种形式.

## 序列建模: 循环和递归网络

```
具体来说, 每当模型能够表示长期依赖时,
长期相互作用的梯度幅值就会变得指数小 (相比短期相互作用的梯度幅值).
这并不意味着这是不可能学习的,
由于长期依赖关系的信号很容易被短期相关性产生的最小波动隐藏,
因而学习长期依赖可能需要很长的时间.
实践中, 实验表明, 当我们增加了需要捕获的依赖关系的跨度,
基于梯度的优化变得越来越困难, SGD 在长度仅为 10 或 20
的序列上成功训练传统 RNN 的概率迅速变为 0.
```

## 实践方法论

- 梯度截断特别重要, 因为它保持了爆炸梯度边缘的 `RNN` 动态.
  如果没有梯度截断, 梯度爆炸将阻碍学习的成功.

| 超参数 | 容量何时增加 | 原因 | 注意事项 |
|-------|:----------:|:---:|:-------:|
| 隐藏单元数量 | 增加 | 增加隐藏单元数量会增加模型的表示能力 | 几乎模型每个操作所需的时间和内存代价都会随隐藏单元数量的增加而增加 |
| 学习率 | 调至最优 | 不正确的学习速率, 不管是太高还是太低都会由于优化失败而导致低有效容量的模型 | |
| 卷积核宽度 | 增加 | 增加卷积核宽度会增加模型的参数数量 | 较宽的卷积核导致较窄的输出尺寸, 除非使用隐式零填充减少此影响, 否则会降低模型容量. 较宽的卷积核需要更多的内存存储参数, 并会增加运行时间, 但较窄的输出会降低内存代价 |
| 隐式零填充 | 增加 | 在卷积之前隐式添加零能保持较大尺寸的表示 | 大多数操作的时间和内存代价会增加 |
| 权重衰减系数 | 降低 | 降低权重衰减系数使得模型参数可以自由地变大 | |
| Dropout 比率 | 降低 | 较少地丢弃单元可以更多地让单元彼此"协力"来适应训练集 | |

```
实际上, 当有几个超参数对性能度量没有显著影响时,
随机搜索相比于网格搜索指数级地高效.
相比于网格搜索, 随机搜索能够更快地减小验证集误差
(就每个模型运行的试验数而言).
与网格搜索一样, 我们通常会重复运行不同版本的随机搜索,
以基于前一次运行的结果改进下一次搜索.
```

## 表示学习

- 从自编码器获得有用特征的一种方法是限制 `h` 的维度比 `x` 小,
  这种编码维度小于输入维度的自编码器称为`欠完备`自编码器.
  - 学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征.

```
因此, 拥有非线性编码器函数 f 和非线性解码器函数 g
的自编码器能够学习出更强大的 PCA 非线性推广.
不幸的是, 如果编码器和解码器被赋予过大的容量,
自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息.
```

```
理想情况下, 根据要建模的数据分布的复杂性, 选择合适的编码维数和编码器,
解码器容量, 就可以成功训练任意架构的自编码器. 正则自编码器提供这样的能力.
正则自编码器使用的损失函数可以鼓励模型学习其他特性 (除了将输入复制到输出),
而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量.
这些特性包括稀疏表示, 表示的小导数以及对噪声或输入缺失的鲁棒性.
即使模型容量大到足以学习一个无意义的恒等函数,
非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布的有用信息.
```

## 深度学习中的结构化概率模型

- `有向图模型`是一种结构化概率模型, 也被称为`信念网络`或者`贝叶斯网络`.
- `有向图模型`为我们提供了一种描述结构化概率模型的语言.
  而另一种常见的语言则是`无向模型`, 也被称为`马尔可夫随机场`或者是`马尔可夫网络`.
  - 就像它们的名字所说的那样, 无向模型中所有的边都是没有方向的.

```
当存在很明显的理由画出每一个指向特定方向的箭头时, 有向模型显然最适用.
有向模型中, 经常存在我们理解的具有因果关系以及因果关系有明确方向的情况.
```

```
虽然玻尔兹曼机最初的定义既可以包含潜变量, 也可以不包含潜变量,
但是时至今日玻尔兹曼机这个术语通常用于指拥有潜变量的模型,
而没有潜变量的玻尔兹曼机则经常被称为马尔可夫随机场或对数线性模型.
```

```
换句话说, 基于能量的模型只是一种特殊的马尔可夫网络:
求幂使能量函数中的每个项对应于不同团的一个因子.
```
