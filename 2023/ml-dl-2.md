---
title: 深度学习 (中)
description: 道狭草木长, 夕露沾我衣. 衣沾不足惜, 但使愿无违.
date: 2022-11-01
---

- [深度学习](https://book.douban.com/subject/27087503/)

## 卷积网络

```
卷积是一种特殊的线性运算.
卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络.
```

> ML 中的卷积运算和数学意义上的卷积有差异~

```
卷积运算可交换性的出现是因为我们将核相对输入进行了翻转,
从 m 增大的角度来看, 输入的索引在增大, 但是核的索引在减小.
我们将核翻转的唯一目的是实现可交换性. 尽管可交换性在证明时很有用,
但在神经网络的应用中却不是一个重要的性质. 与之不同的是,
许多神经网络库会实现一个相关的函数, 称为互相关函数,
和卷积运算几乎一样但是并没有对核进行翻转.

许多机器学习的库实现的是互相关函数但是称之为卷积.
在这本书中我们遵循把两种运算都叫作卷积的这个传统,
在与核翻转有关的上下文中, 我们会特别指明是否对核进行了翻转.
```

```
在机器学习中, 学习算法会在核合适的位置学得恰当的值,
所以一个基于核翻转的卷积运算的学习算法所学得的核,
是对未进行翻转的算法学得的核的翻转.
单独使用卷积运算在机器学习中是很少见的, 卷积经常与其他的函数一起使用,
无论卷积运算是否对它的核进行了翻转, 这些函数的组合通常是不可交换的.
```

```
处于卷积网络更深的层中的单元, 它们的接受域要比处在浅层的单元的接受域更大.
如果网络还包含类似步幅卷积或者池化之类的结构特征, 这种效应会加强.
这意味着在卷积网络中尽管直接连接都是很稀疏的,
但处在更深的层中的单元可以间接地连接到全部或者大部分输入图像.
```

```
池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出.
例如, 最大池化函数给出相邻矩形区域内的最大值.
其他常用的池化函数包括相邻矩形区域内的平均值,
L2 范数以及基于距中心像素距离的加权平均函数.
```

```
使用池化可以看作增加了一个无限强的先验:
这一层学得的函数必须具有对少量平移的不变性.
当这个假设成立时, 池化可以极大地提高网络的统计效率.
对空间区域进行池化产生了平移不变性,
但当我们对分离参数的卷积的输出进行池化时,
特征能够学得应该对于哪种变换具有不变性.
```

```
我们可以把卷积网络类比成全连接网络, 但对于这个全连接网络的权重有一个无限强的先验.
这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同, 但可以在空间上移动.
这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重以外,
其余的权重都为零.
总之, 我们可以把卷积的使用当作对网络中一层的参数引入了一个无限强的先验概率分布.
这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性.
类似地, 使用池化也是一个无限强的先验: 每一个单元都具有对少量平移的不变性.
```

```
当我们知道每一个特征都是一小块空间的函数并且相同的特征不会出现在所有的空间上时,
局部连接层是很有用的.
```

```
平铺卷积对卷积层和局部连接层进行了折衷.
这里并不是对每一个空间位置的权重集合进行学习,
我们学习一组核使得当我们在空间移动时它们可以循环利用.
这意味着在近邻的位置上拥有不同的过滤器, 就像局部连接层一样,
但是对于这些参数的存储需求仅仅会增长常数倍,
这个常数就是核的集合的大小, 而不是整个输出的特征映射的大小.
```

- 卷积等效于使用傅里叶变换将输入与核都转换到频域, 执行两个信号的逐点相乘,
  再使用傅里叶逆变换转换回时域. 对于某些问题的规模,
  这种算法可能比离散卷积的朴素实现更快.
- 当一个 `d` 维的核可以表示成 `d` 个向量 (每一维一个向量) 的外积时,
  该核被称为可分离的. 当核可分离时, 朴素的卷积是低效的.
  它等价于组合 `d` 个一维卷积, 每个卷积使用这些向量中的一个.
  - 组合方法显著快于使用它们的外积来执行一个 `d` 维的卷积,
    并且核也只要更少的参数来表示成向量.
  - 如果核在每一维都是 `w` 个元素宽, 那么朴素的多维卷积需要
    $$ O(w^d) $$
    的运行时间和参数存储空间, 而可分离卷积只需要
    $$ O(w \times d) $$
    的运行时间和参数存储空间.
  - 当然, 并不是每个卷积都可以表示成这种形式.

## 序列建模: 循环和递归网络

```
具体来说, 每当模型能够表示长期依赖时,
长期相互作用的梯度幅值就会变得指数小 (相比短期相互作用的梯度幅值).
这并不意味着这是不可能学习的,
由于长期依赖关系的信号很容易被短期相关性产生的最小波动隐藏,
因而学习长期依赖可能需要很长的时间.
实践中, 实验表明, 当我们增加了需要捕获的依赖关系的跨度,
基于梯度的优化变得越来越困难, SGD 在长度仅为 10 或 20
的序列上成功训练传统 RNN 的概率迅速变为 0.
```

## 实践方法论

- 梯度截断特别重要, 因为它保持了爆炸梯度边缘的 `RNN` 动态.
  如果没有梯度截断, 梯度爆炸将阻碍学习的成功.

| 超参数 | 容量何时增加 | 原因 | 注意事项 |
|-------|:----------:|:---:|:-------:|
| 隐藏单元数量 | 增加 | 增加隐藏单元数量会增加模型的表示能力 | 几乎模型每个操作所需的时间和内存代价都会随隐藏单元数量的增加而增加 |
| 学习率 | 调至最优 | 不正确的学习速率, 不管是太高还是太低都会由于优化失败而导致低有效容量的模型 | |
| 卷积核宽度 | 增加 | 增加卷积核宽度会增加模型的参数数量 | 较宽的卷积核导致较窄的输出尺寸, 除非使用隐式零填充减少此影响, 否则会降低模型容量. 较宽的卷积核需要更多的内存存储参数, 并会增加运行时间, 但较窄的输出会降低内存代价 |
| 隐式零填充 | 增加 | 在卷积之前隐式添加零能保持较大尺寸的表示 | 大多数操作的时间和内存代价会增加 |
| 权重衰减系数 | 降低 | 降低权重衰减系数使得模型参数可以自由地变大 | |
| Dropout 比率 | 降低 | 较少地丢弃单元可以更多地让单元彼此"协力"来适应训练集 | |

```
实际上, 当有几个超参数对性能度量没有显著影响时,
随机搜索相比于网格搜索指数级地高效.
相比于网格搜索, 随机搜索能够更快地减小验证集误差
(就每个模型运行的试验数而言).
与网格搜索一样, 我们通常会重复运行不同版本的随机搜索,
以基于前一次运行的结果改进下一次搜索.
```

## 线性因子模型

## 自编码器

## 表示学习
