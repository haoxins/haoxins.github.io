```
实际上, 当有几个超参数对性能度量没有显著影响时,
随机搜索相比于网格搜索指数级地高效.
相比于网格搜索, 随机搜索能够更快地减小验证集误差
(就每个模型运行的试验数而言).
与网格搜索一样, 我们通常会重复运行不同版本的随机搜索,
以基于前一次运行的结果改进下一次搜索.
```

## 表示学习

- 从自编码器获得有用特征的一种方法是限制 `h` 的维度比 `x` 小,
  这种编码维度小于输入维度的自编码器称为`欠完备`自编码器.
  - 学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征.

```
因此, 拥有非线性编码器函数 f 和非线性解码器函数 g
的自编码器能够学习出更强大的 PCA 非线性推广.
不幸的是, 如果编码器和解码器被赋予过大的容量,
自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息.
```

```
理想情况下, 根据要建模的数据分布的复杂性, 选择合适的编码维数和编码器,
解码器容量, 就可以成功训练任意架构的自编码器. 正则自编码器提供这样的能力.
正则自编码器使用的损失函数可以鼓励模型学习其他特性 (除了将输入复制到输出),
而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量.
这些特性包括稀疏表示, 表示的小导数以及对噪声或输入缺失的鲁棒性.
即使模型容量大到足以学习一个无意义的恒等函数,
非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布的有用信息.
```

## 深度学习中的结构化概率模型

- `有向图模型`是一种结构化概率模型, 也被称为`信念网络`或者`贝叶斯网络`.
- `有向图模型`为我们提供了一种描述结构化概率模型的语言.
  而另一种常见的语言则是`无向模型`, 也被称为`马尔可夫随机场`或者是`马尔可夫网络`.
  - 就像它们的名字所说的那样, 无向模型中所有的边都是没有方向的.

```
当存在很明显的理由画出每一个指向特定方向的箭头时, 有向模型显然最适用.
有向模型中, 经常存在我们理解的具有因果关系以及因果关系有明确方向的情况.
```

```
虽然玻尔兹曼机最初的定义既可以包含潜变量, 也可以不包含潜变量,
但是时至今日玻尔兹曼机这个术语通常用于指拥有潜变量的模型,
而没有潜变量的玻尔兹曼机则经常被称为马尔可夫随机场或对数线性模型.
```

```
换句话说, 基于能量的模型只是一种特殊的马尔可夫网络:
求幂使能量函数中的每个项对应于不同团的一个因子.
```
