```
由于 AROPE 仅需要在原始的稀疏邻接矩阵 A 上计算一次特征分解,
其时间复杂度与图规模呈线性关系, 可以扩展到大规模图. 此外,
在保持不同阶邻近度时, AROPE
仅需计算对应的变换而无须重新计算特征值分解或奇异值分解,
因此该算法可以在不同阶邻近度间快速切换,
以支持高效并有效地保持不同阶邻近度.
```

```
因此, 基于随机游走的图嵌入方法等价为构造特殊的相似度矩阵并计算矩阵分解.
一方面由于随机游走过程不需要显式地构造相似度并计算其矩阵分解,
因此随机游走方法的计算效率往往较高; 另一方面,
由于现实中随机游走的数量是有限的,
因此随机游走方法相当于在优化过程中进行了近似,
而直接采用矩阵分解方法可以更有效地优化目标函数.
```

### 图神经网络

```
概括来说, 卷积定理证明, 函数卷积的傅里叶变换是函数傅里叶变换的乘积.
因此, 利用谱图理论可以将该定理扩展到图数据上,
即两个图信号的卷积是其图傅里叶变换的乘积.
所以, 图卷积操作等价为如下过程:
首先利用图傅里叶变换将图上节点特征 (即若干图信号) 从空域转换到谱域,
在谱域与可学习的滤波器进行乘积操作, 然后再将处理后的特征通过图傅里叶逆变换转换回空域,
即得到处理后的图信号. 其中, 谱域图神经网络的可学习参数与谱域滤波器相关.
类比图像上的卷积神经网络, 上述过程均是可微分的,
因此谱域图神经网络可以实现图数据上端到端的学习.
```

```
谱域图卷积定义为三个步骤:
(1) 通过图傅里叶变换将两组图信号从空域转换到谱域;
(2) 在谱域上计算向量点乘;
(3) 将处理后的图信号经过图傅里叶逆变换转换回空域上.
```

```
与切比雪夫图卷积神经网络类似, 图卷积神经网络不需要显式地计算拉普拉斯矩阵的特征分解,
也无须切比雪夫图卷积神经网络中的递归计算, 因此图卷积神经网络的计算效率很高.
从图卷积神经网络的计算公式可以看出, 其计算复杂度与图中边的数量呈线性关系.
此外, 由于图卷积神经网络可被写为简单的矩阵形式, 其编程实现也非常便捷.

在每一层图卷积中, 每个节点的表征仅与其邻居节点相关,
因此图卷积神经网络有很强的空间局部性. 事实上,
图卷积神经网络也可从空域图神经网络角度理解,
从而将谱域图神经网络与空域图神经网络有效地联系到了一起.
```

```
由于消息传递图神经网络的所有操作都是连续可微分的,
因此消息传递图神经网络可以进行端到端的学习.
不难看出, 与前文介绍的空域图神经网络类似,
消息传递图神经网络也是通过在图的邻居结构上定义函数,
聚合邻居信息以更新节点表征的.
```

```
层次图池化模型的核心思想是逐步"粗化"处理图,
即有序减小图的规模, 并保留图的层次结构信息,
直至学习到所需要颗粒度的图表征.
```

### 图表征学习理论分析

```
谱域中的低频分量和高频分量也可能分别反映了图在空域中的局部结构信息和全局结构信息.
基于上述原因, 仅考虑低通滤波的图神经网络可能在许多图任务中存在局限,
这也促使很多研究者开始探索如何从图信号处理的角度来设计超越低通滤波器的图神经网络.
```

```
当每个节点可被唯一识别时, 图神经网络的消息传递机制可以更有效地传递信息.
然而, 对于现实中的图, 其经常无法保证节点可被唯一识别,
例如在图同构的 WL 测试中, 通常假设所有节点均无任何节点特征.
在这种情况下, 图神经网络的通用逼近性则无法保证. 此外,
利用节点编号等方式人工设置特征作为节点唯一识别往往是不可行的,
因为其无法满足图的置换等变性.
```
